{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.6.5"
    },
    "colab": {
      "name": "20200824_plus_layer8_dropout_kullback_leibler_divergence.ipynb",
      "provenance": [],
      "collapsed_sections": [
        "3ArNio6RRgTe"
      ],
      "toc_visible": true,
      "include_colab_link": true
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/leekate/2020ctrc/blob/master/20200824_plus_layer8_dropout_kullback_leibler_divergence.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kBpq41JKVj6M",
        "colab_type": "text"
      },
      "source": [
        "# **MULTI_IMAGE_CLASSIFICATION**\n",
        "\n",
        "4가지의 이미지 분류\n",
        "\n",
        "기본적인 방법은 단일 이미지 분류와 같다. 대신, 다중 이미지 분류이기 때문에 카테고리의 변화가 있다.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Gyxw6i29yifu",
        "colab_type": "text"
      },
      "source": [
        "## 1번 train"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Z7raZpBpWzvc",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 141
        },
        "outputId": "fd681101-523b-4627-ec54-642db04d89f7"
      },
      "source": [
        "from google.colab import drive\n",
        "\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Go to this URL in a browser: https://accounts.google.com/o/oauth2/auth?client_id=947318989803-6bn6qk8qdgf4n4g3pfee6491hc0brc4i.apps.googleusercontent.com&redirect_uri=urn%3aietf%3awg%3aoauth%3a2.0%3aoob&scope=email%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdocs.test%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive.photos.readonly%20https%3a%2f%2fwww.googleapis.com%2fauth%2fpeopleapi.readonly&response_type=code\n",
            "\n",
            "Enter your authorization code:\n",
            "··········\n",
            "Mounted at /content/drive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "D6L9_lOxrLTr",
        "colab_type": "text"
      },
      "source": [
        "### 파일 읽어오기"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VlpQzeS_9yzx",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "183a3e7e-79f0-444e-fc1f-42ad0c52711c"
      },
      "source": [
        "!pip uninstall keras\n",
        "!pip install Keras==2.2.4\n",
        "\n",
        "!pip uninstall tensorflow\n",
        "!pip install tensorflow==1.13.1"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Uninstalling Keras-2.4.3:\n",
            "  Would remove:\n",
            "    /usr/local/lib/python3.6/dist-packages/Keras-2.4.3.dist-info/*\n",
            "    /usr/local/lib/python3.6/dist-packages/docs/*\n",
            "    /usr/local/lib/python3.6/dist-packages/keras/*\n",
            "  Would not remove (might be manually added):\n",
            "    /usr/local/lib/python3.6/dist-packages/docs/md_autogen.py\n",
            "    /usr/local/lib/python3.6/dist-packages/docs/update_docs.py\n",
            "Proceed (y/n)? y\n",
            "  Successfully uninstalled Keras-2.4.3\n",
            "Collecting Keras==2.2.4\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/5e/10/aa32dad071ce52b5502266b5c659451cfd6ffcbf14e6c8c4f16c0ff5aaab/Keras-2.2.4-py2.py3-none-any.whl (312kB)\n",
            "\u001b[K     |████████████████████████████████| 317kB 7.9MB/s \n",
            "\u001b[?25hRequirement already satisfied: h5py in /usr/local/lib/python3.6/dist-packages (from Keras==2.2.4) (2.10.0)\n",
            "Requirement already satisfied: keras-preprocessing>=1.0.5 in /usr/local/lib/python3.6/dist-packages (from Keras==2.2.4) (1.1.2)\n",
            "Collecting keras-applications>=1.0.6\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/71/e3/19762fdfc62877ae9102edf6342d71b28fbfd9dea3d2f96a882ce099b03f/Keras_Applications-1.0.8-py3-none-any.whl (50kB)\n",
            "\u001b[K     |████████████████████████████████| 51kB 5.9MB/s \n",
            "\u001b[?25hRequirement already satisfied: six>=1.9.0 in /usr/local/lib/python3.6/dist-packages (from Keras==2.2.4) (1.15.0)\n",
            "Requirement already satisfied: pyyaml in /usr/local/lib/python3.6/dist-packages (from Keras==2.2.4) (3.13)\n",
            "Requirement already satisfied: scipy>=0.14 in /usr/local/lib/python3.6/dist-packages (from Keras==2.2.4) (1.4.1)\n",
            "Requirement already satisfied: numpy>=1.9.1 in /usr/local/lib/python3.6/dist-packages (from Keras==2.2.4) (1.18.5)\n",
            "Installing collected packages: keras-applications, Keras\n",
            "Successfully installed Keras-2.2.4 keras-applications-1.0.8\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.colab-display-data+json": {
              "pip_warning": {
                "packages": [
                  "keras"
                ]
              }
            }
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "Uninstalling tensorflow-2.3.0:\n",
            "  Would remove:\n",
            "    /usr/local/bin/estimator_ckpt_converter\n",
            "    /usr/local/bin/saved_model_cli\n",
            "    /usr/local/bin/tensorboard\n",
            "    /usr/local/bin/tf_upgrade_v2\n",
            "    /usr/local/bin/tflite_convert\n",
            "    /usr/local/bin/toco\n",
            "    /usr/local/bin/toco_from_protos\n",
            "    /usr/local/lib/python3.6/dist-packages/tensorflow-2.3.0.dist-info/*\n",
            "    /usr/local/lib/python3.6/dist-packages/tensorflow/*\n",
            "Proceed (y/n)? \n",
            "Your response ('') was not one of the expected responses: y, n\n",
            "Proceed (y/n)? y\n",
            "  Successfully uninstalled tensorflow-2.3.0\n",
            "Collecting tensorflow==1.13.1\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/77/63/a9fa76de8dffe7455304c4ed635be4aa9c0bacef6e0633d87d5f54530c5c/tensorflow-1.13.1-cp36-cp36m-manylinux1_x86_64.whl (92.5MB)\n",
            "\u001b[K     |████████████████████████████████| 92.5MB 107kB/s \n",
            "\u001b[?25hRequirement already satisfied: keras-preprocessing>=1.0.5 in /usr/local/lib/python3.6/dist-packages (from tensorflow==1.13.1) (1.1.2)\n",
            "Requirement already satisfied: keras-applications>=1.0.6 in /usr/local/lib/python3.6/dist-packages (from tensorflow==1.13.1) (1.0.8)\n",
            "Requirement already satisfied: wheel>=0.26 in /usr/local/lib/python3.6/dist-packages (from tensorflow==1.13.1) (0.34.2)\n",
            "Collecting tensorflow-estimator<1.14.0rc0,>=1.13.0\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/bb/48/13f49fc3fa0fdf916aa1419013bb8f2ad09674c275b4046d5ee669a46873/tensorflow_estimator-1.13.0-py2.py3-none-any.whl (367kB)\n",
            "\u001b[K     |████████████████████████████████| 368kB 48.0MB/s \n",
            "\u001b[?25hRequirement already satisfied: astor>=0.6.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow==1.13.1) (0.8.1)\n",
            "Requirement already satisfied: termcolor>=1.1.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow==1.13.1) (1.1.0)\n",
            "Requirement already satisfied: six>=1.10.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow==1.13.1) (1.15.0)\n",
            "Collecting tensorboard<1.14.0,>=1.13.0\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/0f/39/bdd75b08a6fba41f098b6cb091b9e8c7a80e1b4d679a581a0ccd17b10373/tensorboard-1.13.1-py3-none-any.whl (3.2MB)\n",
            "\u001b[K     |████████████████████████████████| 3.2MB 43.6MB/s \n",
            "\u001b[?25hRequirement already satisfied: numpy>=1.13.3 in /usr/local/lib/python3.6/dist-packages (from tensorflow==1.13.1) (1.18.5)\n",
            "Requirement already satisfied: absl-py>=0.1.6 in /usr/local/lib/python3.6/dist-packages (from tensorflow==1.13.1) (0.9.0)\n",
            "Requirement already satisfied: gast>=0.2.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow==1.13.1) (0.3.3)\n",
            "Requirement already satisfied: grpcio>=1.8.6 in /usr/local/lib/python3.6/dist-packages (from tensorflow==1.13.1) (1.31.0)\n",
            "Requirement already satisfied: protobuf>=3.6.1 in /usr/local/lib/python3.6/dist-packages (from tensorflow==1.13.1) (3.12.4)\n",
            "Requirement already satisfied: h5py in /usr/local/lib/python3.6/dist-packages (from keras-applications>=1.0.6->tensorflow==1.13.1) (2.10.0)\n",
            "Collecting mock>=2.0.0\n",
            "  Downloading https://files.pythonhosted.org/packages/cd/74/d72daf8dff5b6566db857cfd088907bb0355f5dd2914c4b3ef065c790735/mock-4.0.2-py3-none-any.whl\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.6/dist-packages (from tensorboard<1.14.0,>=1.13.0->tensorflow==1.13.1) (3.2.2)\n",
            "Requirement already satisfied: werkzeug>=0.11.15 in /usr/local/lib/python3.6/dist-packages (from tensorboard<1.14.0,>=1.13.0->tensorflow==1.13.1) (1.0.1)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.6/dist-packages (from protobuf>=3.6.1->tensorflow==1.13.1) (49.2.0)\n",
            "Requirement already satisfied: importlib-metadata; python_version < \"3.8\" in /usr/local/lib/python3.6/dist-packages (from markdown>=2.6.8->tensorboard<1.14.0,>=1.13.0->tensorflow==1.13.1) (1.7.0)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.6/dist-packages (from importlib-metadata; python_version < \"3.8\"->markdown>=2.6.8->tensorboard<1.14.0,>=1.13.0->tensorflow==1.13.1) (3.1.0)\n",
            "Installing collected packages: mock, tensorflow-estimator, tensorboard, tensorflow\n",
            "  Found existing installation: tensorflow-estimator 2.3.0\n",
            "    Uninstalling tensorflow-estimator-2.3.0:\n",
            "      Successfully uninstalled tensorflow-estimator-2.3.0\n",
            "  Found existing installation: tensorboard 2.3.0\n",
            "    Uninstalling tensorboard-2.3.0:\n",
            "      Successfully uninstalled tensorboard-2.3.0\n",
            "Successfully installed mock-4.0.2 tensorboard-1.13.1 tensorflow-1.13.1 tensorflow-estimator-1.13.0\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.colab-display-data+json": {
              "pip_warning": {
                "packages": [
                  "tensorboard",
                  "tensorflow"
                ]
              }
            }
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Esqlk-sO4H93",
        "colab_type": "text"
      },
      "source": [
        "### 폴더내 이미지 개수 확인하기 -validation\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "e5QppKAj4L7V",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "2ba59ca7-6edf-4795-e9a0-f50db9190842"
      },
      "source": [
        "import os\n",
        "\n",
        "path, dirs, files = next(os.walk(\"/content/drive/My Drive/CTRC/validation/1. Cancer\"))\n",
        "file_count = len(files)\n",
        "\n",
        "file_count"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "410"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 1
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IdQlCbkh4WNB",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "a93b6d1e-6f4a-4e64-d8e4-746e97907863"
      },
      "source": [
        "import os\n",
        "\n",
        "path, dirs, files = next(os.walk(\"/content/drive/My Drive/CTRC/validation/2. Precancer\"))\n",
        "file_count = len(files)\n",
        "\n",
        "file_count"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "150"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 2
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zbfoUq-I4WKQ",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "da3ea069-0160-4c86-989c-1c6821aa29c3"
      },
      "source": [
        "import os\n",
        "\n",
        "path, dirs, files = next(os.walk(\"/content/drive/My Drive/CTRC/validation/3. Extra\"))\n",
        "file_count = len(files)\n",
        "\n",
        "file_count"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "265"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-wW2AHTN4n7F",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "4e5d49cb-ef16-465f-db03-e686d9b02330"
      },
      "source": [
        "import os\n",
        "\n",
        "path, dirs, files = next(os.walk(\"/content/drive/My Drive/CTRC/validation/4. Normal\"))\n",
        "file_count = len(files)\n",
        "\n",
        "file_count"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "1137"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lXf8Dpe1KeHg",
        "colab_type": "text"
      },
      "source": [
        "### 데이터 부풀리기"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XcF6Pi6w4n47",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nC2ngZOO4Q3a",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 268
        },
        "outputId": "14012bae-7391-4168-d66d-d77add72e6c9"
      },
      "source": [
        "import os\n",
        "from tensorflow.keras.preprocessing.image import ImageDataGenerator,array_to_img,img_to_array, load_img\n",
        "import numpy as np\n",
        "\n",
        "np.random.seed(3)\n",
        "\n",
        "imageGenerator = ImageDataGenerator(rescale=1./255,\n",
        "                                    rotation_range=20,\n",
        "                                    width_shift_range=0.1,\n",
        "                                    height_shift_range=0.1,\n",
        "                                    brightness_range=[.2,.2],\n",
        "                                    horizontal_flip=True)\n",
        "                                    #validation_split=.2\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/dtypes.py:526: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
            "  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n",
            "/usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/dtypes.py:527: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
            "  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n",
            "/usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/dtypes.py:528: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
            "  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n",
            "/usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/dtypes.py:529: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
            "  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n",
            "/usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/dtypes.py:530: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
            "  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n",
            "/usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/dtypes.py:535: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
            "  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HUXM1JyVFX9D",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 74
        },
        "outputId": "158d0403-d3c9-4854-d193-a09e71161cf5"
      },
      "source": [
        "#데이터가 충분하면 나누기\n",
        "trainGen = imageGenerator.flow_from_directory('/content/drive/My Drive/CTRC/train',\n",
        "                                              target_size=(64,64),\n",
        "                                              subset='training')\n",
        "\n",
        "'''validationGen = imageGenerator.flow_from_directory('/content/drive/My Drive/CTRC/train',\n",
        "                                                  target_size=(64,64),\n",
        "                                                  subset='validation')'''"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Found 11762 images belonging to 4 classes.\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "\"validationGen = imageGenerator.flow_from_directory('/content/drive/My Drive/CTRC/train',\\n                                                  target_size=(64,64),\\n                                                  subset='validation')\""
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 2
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MXjtASprMCzx",
        "colab_type": "text"
      },
      "source": [
        "\n",
        "\n",
        "*   rotation_range = 90: \n",
        "지정된 각도 범위내에서 임의로 원본이미지를 회전시킵니다. 단위는 도이며, 정수형입니다. 예를 들어 90이라면 0도에서 90도 사이에 임의의 각도로 회전시킵니다. \n",
        "*   width_shift_range = 0.1: \n",
        "지정된 수평방향 이동 범위내에서 임의로 원본이미지를 이동시킵니다. 수치는 전체 넓이의 비율(실수)로 나타냅니다. 예를 들어 0.1이고 전체 넓이가 100이면, 10픽셀 내외로 좌우 이동시킵니다\n",
        "\n",
        "*   height_shift_range = 0.1: \n",
        "지정된 수직방향 이동 범위내에서 임의로 원본이미지를 이동시킵니다. 수치는 전체 높이의 비율(실수)로 나타냅니다. 예를 들어 0.1이고 전체 높이가 100이면, 10픽셀 내외로 상하 이동시킵니다.\n",
        "*   shear_range = 0.5:\n",
        "밀림 강도 범위내에서 임의로 원본이미지를 변형시킵니다. 수치는 시계반대방향으로 밀림 강도를 라디안으로 나타냅니다. 예를 들어 0.5이라면, 0.5 라이안내외로 시계반대방향으로 변형시킵니다.\n",
        "\n",
        "*   zoom_range = 0.3:\n",
        "지정된 확대/축소 범위내에서 임의로 원본이미지를 확대/축소합니다. “1-수치”부터 “1+수치”사이 범위로 확대/축소를 합니다. 예를 들어 0.3이라면, 0.7배에서 1.3배 크기 변화를 시킵니다.\n",
        "*   horizontal_flip = True:\n",
        "수평방향으로 뒤집기를 합니다.\n",
        "*   vertical_flip = True:\n",
        "수직방향으로 뒤집기를 합니다.\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lPGhbtSmDVDT",
        "colab_type": "text"
      },
      "source": [
        "1. Cancer"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nqK1bkW6-FlV",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c0914f6a-10d6-42ca-c5a7-b775eea82419"
      },
      "source": [
        "import os\n",
        "import numpy as np\n",
        "from PIL import Image\n",
        "#path = 이미지resize할 file directory\n",
        "path1 = \"/content/drive/My Drive/CTRC/validation/1. Cancer\"\n",
        "\n",
        "file_list = os.listdir(path1)\n",
        "\n",
        "print (\"file_list: {}\".format(file_list))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "file_list: ['can_1.jpg', 'can_2.jpg', 'can_3.jpg', 'can_4.jpg', 'can_5.jpg', 'can_6.jpg', 'can_8.jpg', 'can_9.jpg', 'can_10.jpg', 'can_11.jpg', 'can_12.jpg', 'can_13.jpg', 'can_14.jpg', 'can_15.jpg', 'can_15_1.jpg', 'can_16.jpg', 'can_17.jpg', 'can_17_1.jpg', 'can_18.jpg', 'can_19.jpg', 'can_20.jpg', 'can_21.jpg', 'can_22.jpg', 'can_22_1.jpg', 'can_23.jpg', 'can_24.jpg', 'can_25.jpg', 'can_26.jpg', 'can_27.jpg', 'can_28.jpg', 'can_30.jpg', 'can_29.jpg', 'can_31.jpg', 'can_32.jpg', 'can_32_1.jpg', 'can_33.jpg', 'can_34.jpg', 'can_35.jpg', 'can_36.jpg', 'can_37.jpg', 'can_37_1.jpg', 'can_38.jpg', 'can_39.jpg', 'can_40.jpg', 'can_40_1.jpg', 'can_41.jpg', 'can_42.jpg', 'can_43.jpg', 'can_44.jpg', 'can_51.jpg', 'can_52.jpg', 'can_53.jpg', 'can_53_1.jpg', 'can_53_2.jpg', 'can_55.jpg', 'can_54.jpg', 'can_57.jpg', 'can_56.jpg', 'can_58.jpg', 'can_59.jpg', 'can_59_1.jpg', 'can_59_2.jpg', 'can_63.jpg', 'can_60.jpg', 'can_60_1.jpg', 'can_62.jpg', 'can_61.jpg', 'can_61_1.jpg', 'can_45.jpg', 'can_46.jpg', 'can_47.jpg', 'can_47_1.jpg', 'can_48.jpg', 'can_49.jpg', 'can_50.jpg', 'can_64.jpg', 'can_65.jpg', 'can_66.jpg', 'can_67.jpg', 'can_67_1.jpg', 'can_67_2.jpg', 'can_68.jpg', 'can_69.jpg', 'can_70.jpg', 'can_71.jpg', 'can_72.jpg', 'can_73.jpg', 'can_74.jpg', 'can_75.jpg', 'can_76.jpg', 'can_77.jpg', 'can_78.jpg', 'can_79.jpg', 'can_80.jpg', 'can_82.jpg', 'can_81.jpg', 'can_83.jpg', 'can_84.jpg', 'can_86.jpg', 'can_85.jpg', 'can_87.jpg', 'can_88.jpg', 'can_89.jpg', 'can_90.jpg', 'can_91.jpg', 'can_92.jpg', 'can_93.jpg', 'can_94.jpg', 'can_95.jpg', 'can_96.jpg', 'can_97.jpg', 'can_97_1.jpg', 'can_98.jpg', 'can_99.jpg', 'can_100.jpg', 'can_101.jpg', 'can_102.jpg', 'can_103.jpg', 'can_104.jpg', 'can_105.jpg', 'can_106.jpg', 'can_107.jpg', 'can_108.jpg', 'can_109.jpg', 'can_110.jpg', 'can_111.jpg', 'can_112.jpg', 'can_113.jpg', 'can_114.jpg', 'can_115.jpg', 'can_116.jpg', 'can_117.jpg', 'can_118.jpg', 'can_120.jpg', 'can_119.jpg', 'can_121.jpg', 'can_122.jpg', 'can_123.jpg', 'can_124.jpg', 'can_125.jpg', 'can_127.jpg', 'can_128.jpg', 'can_129.jpg', 'can_130.jpg', 'can_131.jpg', 'can_132.jpg', 'can_133.jpg', 'can_134.jpg', 'can_135.jpg', 'can_136.jpg', 'can_137.jpg', 'can_138.jpg', 'can_139.jpg', 'can_140.jpg', 'can_141.jpg', 'can_142.jpg', 'can_143.jpg', 'can_144.jpg', 'can_145.jpg', 'can_146.jpg', 'can_147.jpg', 'can_148.jpg', 'can_149.jpg', 'can_150.jpg', 'can_151.jpg', 'can_152.jpg', 'can_153.jpg', 'can_154.jpg', 'can_155.jpg', 'can_156.jpg', 'can_157.jpg', 'can_158.jpg', 'can_159.jpg', 'can_160.jpg', 'can_161.jpg', 'can_162.jpg', 'can_163.jpg', 'can_164.jpg', 'can_165.jpg', 'can_166.jpg', 'can_167.jpg', 'can_168.jpg', 'can_169.jpg', 'can_170.jpg', 'can_171.jpg', 'can_172.jpg', 'can_173.jpg', 'can_174.jpg', 'can_175.jpg', 'can_176.jpg', 'can_177.jpg', 'can_178.jpg', 'can_179.jpg', 'can_180.jpg', 'can_181.jpg', 'can_182.jpg', 'can_183.jpg', 'can_184.jpg', 'can_185.jpg', 'can_186.jpg', 'can_187.jpg', 'can_188.jpg', 'can_189.jpg', 'can_190.jpg', 'can_191.jpg', 'can_192.jpg', 'can_193.jpg', 'can_194.jpg', 'can_195.jpg', 'can_196.jpg', 'can_197.jpg', 'can_198.jpg', 'can_199.jpg', 'can_200.jpg', 'can_201.jpg', 'can_202.jpg', 'can_203.jpg', 'can_204.jpg', 'can_205.jpg', 'can_206.jpg', 'can_207.jpg', 'can_208.jpg', 'can_209.jpg', 'can_210.jpg', 'can_211.jpg', 'can_212.jpg', 'can_213.jpg', 'can_214.jpg', 'can_215.jpg', 'can_216.jpg', 'can_217.jpg', 'can_218.jpg', 'can_219.jpg', 'can_220.jpg', 'can_221.jpg', 'can_222.jpg', 'can_223.jpg', 'can_224.jpg', 'can_225.jpg', 'can_226.jpg', 'can_227.jpg', 'can_228.jpg', 'can_229.jpg', 'can_230.jpg', 'can_231.jpg', 'can_232.jpg', 'can_233.jpg', 'can_234.jpg', 'can_235.jpg', 'can_236.jpg', 'can_237.jpg', 'can_238.jpg', 'can_239.jpg', 'can_240.jpg', 'can_241.jpg', 'can_242.jpg', 'can_243.jpg', 'can_244.jpg', 'can_245.jpg', 'can_246.jpg', 'can_247.jpg', 'can_248.jpg', 'can_249.jpg', 'can_250.jpg', 'can_251.jpg', 'can_252.jpg', 'can_253.jpg', 'can_254.jpg', 'can_255.jpg', 'can_256.jpg', 'can_257.jpg', 'can_258.jpg', 'can_259.jpg', 'can_260.jpg', 'can_261.jpg', 'can_262.jpg', 'can_263.jpg', 'can_264.jpg', 'can_265.jpg', 'can_266.jpg', 'can_267.jpg', 'can_269.jpg', 'can_268.jpg', 'can_270.jpg', 'can_271.jpg', 'can_272.jpg', 'can_273.jpg', 'can_274.jpg', 'can_275.jpg', 'can_276.jpg', 'can_277.jpg', 'can_278.jpg', 'can_279.jpg', 'can_280.jpg', 'can_281.jpg', 'can_282.jpg', 'can_283.jpg', 'can_284.jpg', 'can_285.jpg', 'can_286.jpg', 'can_287.jpg', 'can_288.jpg', 'can_289.jpg', 'can_290.jpg', 'can_291.jpg', 'can_292.jpg', 'can_293.jpg', 'can_294.jpg', 'can_295.jpg', 'can_296.jpg', 'can_297.jpg', 'can_298.jpg', 'can_299.jpg', 'can_300.jpg', 'can_301.jpg', 'can_302.jpg', 'can_303.jpg', 'can_304.jpg', 'can_305.jpg', 'can_306.jpg', 'can_308.jpg', 'can_309.jpg', 'can_310.jpg', 'can_311.jpg', 'can_312.jpg', 'can_313.jpg', 'can_314.jpg', 'can_315.jpg', 'can_316.jpg', 'can_317.jpg', 'can_318.jpg', 'can_319.jpg', 'can_320.jpg', 'can_321.jpg', 'can_322.jpg', 'can_323.jpg', 'can_324.jpg', 'can_325.jpg', 'can_327.jpg', 'can_328.jpg', 'can_329.jpg', 'can_330.jpg', 'can_331.jpg', 'can_332.jpg', 'can_333.jpg', 'can_334.jpg', 'can_335.jpg', 'can_336.jpg', 'can_337.jpg', 'can_338.jpg', 'can_339.jpg', 'can_307.jpg', 'can_340.jpg', 'can_341.jpg', 'can_342.jpg', 'can_343.jpg', 'can_344.jpg', 'can_345.jpg', 'can_346.jpg', 'can_347.jpg', 'can_348.jpg', 'can_326.jpg', 'can_349.jpg', 'can_350.jpg', 'can_351.jpg', 'can_352.jpg', 'can_353.jpg', 'can_354.jpg', 'can_7.jpg', 'can_355.jpg', 'can_356.jpg', 'can_358.jpg', 'can_359.jpg', 'can_360.jpg', 'can_361.jpg', 'can_362.jpg', 'can_363.jpg', 'can_364.jpg', 'can_365.jpg', 'can_366.jpg', 'can_367_1.jpg', 'can_367_2.jpg', 'can_368.jpg', 'can_369_1.jpg', 'can_369_2.jpg', 'can_370.jpg', 'can_371.jpg', 'can_372.jpg', 'can_373.jpg', 'can_374.jpg', 'can_375.jpg', 'can_376.jpg', 'can_377.jpg', 'can_378.jpg', 'can_379.jpg', 'can_380_1.jpg', 'can_380_2.jpg', 'can_381.jpg', 'can_382.jpg', 'can_383.jpg', 'can_384.jpg', 'can_385.jpg', 'can_386.jpg', 'can_387.jpg', 'can_389_1.jpg', 'can_389_2.jpg', 'can_390.jpg', 'can_391.jpg', 'can_357.jpg', 'can_388.jpg', 'can_392.jpg', 'can_393.jpg']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mjt2R1Oo-W5v",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "be31db6b-3af1-44dc-8e89-f36dcb0aeb75"
      },
      "source": [
        "n=len(file_list)\n",
        "print(\"n=%d\"%(n))\n",
        "print(file_list[0])"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "n=412\n",
            "can_1.jpg\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "djuHu0ik-dXD",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a8c39bcc-24c2-4257-b545-b3216dfde9b5"
      },
      "source": [
        "np.random.seed(5)\n",
        "from keras.preprocessing.image import ImageDataGenerator, array_to_img, img_to_array, load_img\n",
        "\n",
        "\n",
        "#데이터가 충분하면 나누기\n",
        "trainGen = imageGenerator.flow_from_directory('/content/drive/My Drive/CTRC/train',\n",
        "                                              target_size=(64,64),\n",
        "                                              subset='training')\n",
        "\n",
        "validationGen = imageGenerator.flow_from_directory('/content/drive/My Drive/CTRC/validation',\n",
        "                                                  target_size=(64,64),\n",
        "                                                  subset='validation')\n",
        "\n",
        "\n",
        "\n",
        "for j in range(n):\n",
        "    \n",
        "    img = load_img('/content/drive/My Drive/CTRC/validation/1. Cancer/{}'.format(file_list[j]))\n",
        "    x = img_to_array(img)\n",
        "    x = x.reshape((1,) + x.shape)\n",
        "\n",
        "    i = 0\n",
        "\n",
        "    for batch in imageGenerator.flow(x, \n",
        "                                     batch_size=1,\n",
        "                                     save_to_dir='/content/drive/My Drive/CTRC/train/1. Cancer', \n",
        "                                     save_prefix='{}'.format(file_list[j]),\n",
        "                                     save_format='jpg'):\n",
        "\n",
        "        i += 1\n",
        "\n",
        "        if i > 5: \n",
        "\n",
        "            break\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "'''\n",
        "  for batch in imageGenerator.flow(x, batch_size=1,\n",
        "                              save_to_dir=saveDir,\n",
        "                              save_format='jpg'):\n",
        "        i += 1'''"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Found 0 images belonging to 4 classes.\n",
            "Found 0 images belonging to 4 classes.\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "\"\\n  for batch in imageGenerator.flow(x, batch_size=1,\\n                              save_to_dir=saveDir,\\n                              save_format='jpg'):\\n        i += 1\""
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 12
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Zn9l8z9J_PFQ",
        "colab_type": "text"
      },
      "source": [
        "2. Precancer\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZWk-j42J_MQr",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "440c8e5b-9277-4f36-c124-d532cfabceda"
      },
      "source": [
        "import os\n",
        "import numpy as np\n",
        "from PIL import Image\n",
        "#path = 이미지resize할 file directory\n",
        "path2 = \"/content/drive/My Drive/CTRC/validation/2. Precancer\"\n",
        "\n",
        "file_list = os.listdir(path2)\n",
        "\n",
        "print (\"file_list: {}\".format(file_list))\n",
        "\n",
        "n=len(file_list)\n",
        "print(\"n=%d\"%(n))\n",
        "print(file_list[0])"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "file_list: ['precan_1.jpg', 'precan_2.jpg', 'precan_3.jpg', 'precan_4.jpg', 'precan_5.jpg', 'precan_6.jpg', 'precan_7.jpg', 'precan_8.jpg', 'precan_9.jpg', 'precan_10.jpg', 'precan_11.jpg', 'precan_12.jpg', 'precan_13.jpg', 'precan_14.jpg', 'precan_15.jpg', 'precan_16.jpg', 'precan_17.jpg', 'precan_18.jpg', 'precan_19.jpg', 'precan_20.jpg', 'precan_21.jpg', 'precan_22.jpg', 'precan_23.jpg', 'precan_24.jpg', 'precan_25.jpg', 'precan_26.jpg', 'precan_27.jpg', 'precan_28.jpg', 'precan_29.jpg', 'precan_30.jpg', 'precan_31.jpg', 'precan_32.jpg', 'precan_33.jpg', 'precan_34.jpg', 'precan_35.jpg', 'precan_36.jpg', 'precan_37.jpg', 'precan_38.jpg', 'precan_39.jpg', 'precan_40.jpg', 'precan_42.jpg', 'precan_43.jpg', 'precan_41.jpg', 'precan_44.jpg', 'precan_45.jpg', 'precan_46.jpg', 'precan_47.jpg', 'precan_48.jpg', 'precan_49.jpg', 'precan_50.jpg', 'precan_51.jpg', 'precan_52.jpg', 'precan_53.jpg', 'precan_54.jpg', 'precan_55.jpg', 'precan_56.jpg', 'precan_57.jpg', 'precan_58.jpg', 'precan_59.jpg', 'precan_60.jpg', 'precan_62.jpg', 'precan_61.jpg', 'precan_63.jpg', 'precan_64.jpg', 'precan_65.jpg', 'precan_66.jpg', 'precan_67.jpg', 'precan_68.jpg', 'precan_69.jpg', 'precan_70.jpg', 'precan_71.jpg', 'precan_72.jpg', 'precan_73.jpg', 'precan_73_1.jpg', 'precan_74.jpg', 'precan_75.jpg', 'precan_75_1.jpg', 'precan_76.jpg', 'precan_77.jpg', 'precan_78.jpg', 'precan_79.jpg', 'precan_80.jpg', 'precan_81.jpg', 'precan_81_1.jpg', 'precan_82.jpg', 'precan_83.jpg', 'precan_84.jpg', 'precan_85.jpg', 'precan_86.jpg', 'precan_87.jpg', 'precan_88.jpg', 'precan_89.jpg', 'precan_90.jpg', 'precan_91.jpg', 'precan_92.jpg', 'precan_93.jpg', 'precan_94.jpg', 'precan_95.jpg', 'precan_96.jpg', 'precan_97.jpg', 'precan_98.jpg', 'precan_99.jpg', 'precan_100.jpg', 'precan_101.jpg', 'precan_102.jpg', 'precan_103.jpg', 'precan_104.jpg', 'precan_105.jpg', 'precan_106.jpg', 'precan_107.jpg', 'precan_108.jpg', 'precan_109.jpg', 'precan_110.jpg', 'precan_111.jpg', 'precan_112.jpg', 'precan_113.jpg', 'precan_114.jpg', 'precan_115.jpg', 'precan_116.jpg', 'precan_117.jpg', 'precan_118.jpg', 'precan_119.jpg', 'precan_120.jpg', 'precan_121.jpg', 'precan_122.jpg', 'precan_123.jpg', 'precan_124.jpg', 'precan_125.jpg', 'precan_126.jpg', 'precan_127.jpg', 'precan_128.jpg', 'precan_129.jpg', 'precan_130.jpg', 'precan_131.jpg', 'precan_132.jpg', 'precan_133.jpg', 'precan_134.jpg', 'precan_135.jpg', 'precan_136.jpg', 'precan_137.jpg', 'precan_138.jpg', 'precan_139.jpg', 'precan_146.jpg', 'precan_141.jpg', 'precan_147.jpg', 'precan_140.jpg', 'precan_144.jpg', 'precan_145.jpg', 'precan_142.jpg', 'precan_143.jpg']\n",
            "n=150\n",
            "precan_1.jpg\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ELEVzorm_RnA",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b65cf969-75c5-4da0-a1a5-b108026030e2"
      },
      "source": [
        "np.random.seed(5)\n",
        "from keras.preprocessing.image import ImageDataGenerator, array_to_img, img_to_array, load_img\n",
        "\n",
        "\n",
        "#데이터가 충분하면 나누기\n",
        "trainGen = imageGenerator.flow_from_directory('/content/drive/My Drive/CTRC/train/2. Precancer',\n",
        "                                              target_size=(64,64),\n",
        "                                              subset='training')\n",
        "\n",
        "validationGen = imageGenerator.flow_from_directory('/content/drive/My Drive/CTRC/validation/2. Precancer',\n",
        "                                                  target_size=(64,64),\n",
        "                                                  subset='validation')\n",
        "\n",
        "\n",
        "\n",
        "for j in range(n):\n",
        "    \n",
        "    img = load_img('/content/drive/My Drive/CTRC/validation/2. Precancer/{}'.format(file_list[j]))\n",
        "    x = img_to_array(img)\n",
        "    x = x.reshape((1,) + x.shape)\n",
        "\n",
        "    i = 0\n",
        "\n",
        "    for batch in imageGenerator.flow(x, batch_size=1, save_to_dir='/content/drive/My Drive/CTRC/train/2. Precancer',\n",
        "                               save_prefix='{}'.format(file_list[j]), save_format='jpg'):\n",
        "\n",
        "        i += 1\n",
        "\n",
        "        if i >5: \n",
        "\n",
        "            break"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Found 0 images belonging to 0 classes.\n",
            "Found 0 images belonging to 0 classes.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MEdhTRFuDXt6",
        "colab_type": "text"
      },
      "source": [
        "3. Extra"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BM2DjGJPDZke",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "3bb9a3ba-2f35-4c56-d877-95bc14d1f54f"
      },
      "source": [
        "import os\n",
        "import numpy as np\n",
        "from PIL import Image\n",
        "#path = 이미지resize할 file directory\n",
        "path3 = \"/content/drive/My Drive/CTRC/validation/3. Extra\"\n",
        "\n",
        "file_list = os.listdir(path3)\n",
        "\n",
        "print (\"file_list: {}\".format(file_list))\n",
        "\n",
        "n=len(file_list)\n",
        "print(\"n=%d\"%(n))\n",
        "print(file_list[0])"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "file_list: ['inf_1.jpg', 'inf_2.jpg', 'inf_3.jpg', 'inf_4.jpg', 'inf_5.jpg', 'inf_5_1.jpg', 'inf_6.jpg', 'inf_6_1.jpg', 'inf_7.jpg', 'inf_8.jpg', 'inf_9.jpg', 'inf_10.jpg', 'inf_11.jpg', 'inf_11_1.jpg', 'inf_12.jpg', 'inf_13.jpg', 'inf_13_1.jpg', 'inf_14.jpg', 'inf_14_1.jpg', 'inf_15.jpg', 'inf_16.jpg', 'inf_17.jpg', 'inf_18.jpg', 'inf_19.jpg', 'inf_20.jpg', 'inf_21.jpg', 'inf_22.jpg', 'inf_23.jpg', 'inf_23_1.jpg', 'inf_24.jpg', 'inf_24_1.jpg', 'inf_25.jpg', 'inf_25_1.jpg', 'inf_25_2.jpg', 'inf_26.jpg', 'inf_27.jpg', 'inf_28.jpg', 'inf_29.jpg', 'inf_30.jpg', 'inf_31.jpg', 'inf_32.jpg', 'inf_33.jpg', 'inf_34.jpg', 'inf_35.jpg', 'inf_36.jpg', 'inf_37.jpg', 'inf_38.jpg', 'inf_39.jpg', 'inf_40.jpg', 'inf_41.jpg', 'inf_42.jpg', 'inf_43.jpg', 'inf_44.jpg', 'inf_45.jpg', 'inf_45_1.jpg', 'inf_46.jpg', 'inf_47.jpg', 'inf_48.jpg', 'inf_49.jpg', 'inf_50.jpg', 'inf_51.jpg', 'inf_52.jpg', 'inf_53.jpg', 'inf_54.jpg', 'inf_55.jpg', 'inf_56.jpg', 'inf_57.jpg', 'inf_58.jpg', 'inf_59.jpg', 'inf_60.jpg', 'inf_61.jpg', 'inf_62.jpg', 'inf_63.jpg', 'inf_64.jpg', 'inf_65.jpg', 'inf_66.jpg', 'inf_67.jpg', 'inf_68.jpg', 'inf_69.jpg', 'inf_70.jpg', 'inf_71.jpg', 'inf_72.jpg', 'inf_73.jpg', 'inf_74.jpg', 'inf_75.jpg', 'inf_76.jpg', 'inf_77.jpg', 'inf_78.jpg', 'inf_79.jpg', 'inf_80.jpg', 'inf_81.jpg', 'inf_82.jpg', 'inf_83.jpg', 'inf_84.jpg', 'inf_85.jpg', 'inf_86.jpg', 'inf_87.jpg', 'inf_88.jpg', 'inf_89.jpg', 'inf_90.jpg', 'inf_91.jpg', 'inf_92.jpg', 'inf_93.jpg', 'inf_94.jpg', 'inf_95.jpg', 'inf_96.jpg', 'inf_97.jpg', 'inf_98.jpg', 'inf_100.jpg', 'inf_101.jpg', 'inf_102.jpg', 'inf_103.jpg', 'inf_104.jpg', 'inf_105.jpg', 'inf_99.jpg', 'inf_106.jpg', 'inf_107.jpg', 'inf_108.jpg', 'inf_108_1.jpg', 'inf_109.jpg', 'inf_110.jpg', 'inf_111.jpg', 'inf_111_1.jpg', 'inf_112.jpg', 'inf_113.jpg', 'inf_114.jpg', 'inf_115.jpg', 'inf_116.jpg', 'inf_117.jpg', 'inf_118.jpg', 'inf_119.jpg', 'inf_120.jpg', 'inf_121.jpg', 'inf_122.jpg', 'inf_123.jpg', 'inf_124.jpg', 'inf_125.jpg', 'inf_126.jpg', 'inf_127.jpg', 'inf_128.jpg', 'inf_129.jpg', 'inf_130.jpg', 'inf_131.jpg', 'inf_132.jpg', 'inf_133.jpg', 'inf_134.jpg', 'inf_135.jpg', 'inf_136.jpg', 'inf_137.jpg', 'inf_138.jpg', 'inf_139.jpg', 'inf_140.jpg', 'inf_141.jpg', 'inf_142.jpg', 'inf_143.jpg', 'inf_144.jpg', 'inf_145.jpg', 'inf_146.jpg', 'inf_147.jpg', 'inf_148.jpg', 'inf_149.jpg', 'inf_150.jpg', 'inf_151.jpg', 'inf_152.jpg', 'inf_153.jpg', 'inf_154.jpg', 'inf_155.jpg', 'inf_156.jpg', 'inf_157.jpg', 'inf_158.jpg', 'inf_159.jpg', 'inf_160.jpg', 'inf_161.jpg', 'inf_162.jpg', 'inf_163.jpg', 'inf_164.jpg', 'inf_165.jpg', 'inf_166.jpg', 'inf_167.jpg', 'inf_168.jpg', 'inf_169.jpg', 'inf_170.jpg', 'inf_171.jpg', 'inf_172.jpg', 'inf_173.jpg', 'inf_174.jpg', 'inf_175.jpg', 'inf_176.jpg', 'inf_177.jpg', 'inf_178.jpg', 'inf_179.jpg', 'inf_180.jpg', 'inf_181.jpg', 'inf_182.jpg', 'inf_183.jpg', 'inf_184.jpg', 'inf_185.jpg', 'inf_186.jpg', 'inf_187.jpg', 'inf_188.jpg', 'inf_189.jpg', 'inf_190.jpg', 'inf_191.jpg', 'inf_192.jpg', 'inf_193.jpg', 'inf_194.jpg', 'inf_195.jpg', 'inf_196.jpg', 'inf_197.jpg', 'inf_198.jpg', 'inf_199.jpg', 'inf_200.jpg', 'inf_201.jpg', 'inf_202.jpg', 'inf_203.jpg', 'inf_204.jpg', 'inf_205.jpg', 'inf_206.jpg', 'inf_238.jpg', 'inf_239.jpg', 'inf_240.jpg', 'inf_241.jpg', 'inf_242.jpg', 'inf_243.jpg', 'inf_244.jpg', 'inf_245.jpg', 'inf_246.jpg', 'inf_235_2.jpg', 'inf_235_1.jpg', 'inf_234.jpg', 'inf_233.jpg', 'inf_232.jpg', 'inf_231.jpg', 'inf_230.jpg', 'inf_229.jpg', 'inf_228.jpg', 'inf_227.jpg', 'inf_226.jpg', 'inf_225_2.jpg', 'inf_225_1.jpg', 'inf_224.jpg', 'inf_223.jpg', 'inf_222.jpg', 'inf_221.jpg', 'inf_220.jpg', 'inf_219_2.jpg', 'inf_219_1.jpg', 'inf_216.jpg', 'inf_215.jpg', 'inf_214.jpg', 'inf_213.jpg', 'inf_212.jpg', 'inf_211_2.jpg', 'inf_211_1.jpg', 'inf_210.jpg', 'inf_209.jpg', 'inf_208.jpg', 'inf_217.jpg', 'inf_218.jpg', 'inf_237.jpg', 'inf_236.jpg', 'inf_207.jpg']\n",
            "n=262\n",
            "inf_1.jpg\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7rGQ0gGbDeIa",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "8ecb6d29-58ed-4a3d-f862-63e15f9cd726"
      },
      "source": [
        "np.random.seed(5)\n",
        "from keras.preprocessing.image import ImageDataGenerator, array_to_img, img_to_array, load_img\n",
        "\n",
        "\n",
        "#데이터가 충분하면 나누기\n",
        "trainGen = imageGenerator.flow_from_directory('/content/drive/My Drive/CTRC/train/3. Extra',\n",
        "                                              target_size=(64,64),\n",
        "                                              subset='training')\n",
        "\n",
        "validationGen = imageGenerator.flow_from_directory('/content/drive/My Drive/CTRC/validation/3. Extra',\n",
        "                                                  target_size=(64,64),\n",
        "                                                  subset='validation')\n",
        "\n",
        "\n",
        "\n",
        "for j in range(n):\n",
        "    \n",
        "    img = load_img('/content/drive/My Drive/CTRC/validation/3. Extra/{}'.format(file_list[j]))\n",
        "    x = img_to_array(img)\n",
        "    x = x.reshape((1,) + x.shape)\n",
        "\n",
        "    i = 0\n",
        "\n",
        "    for batch in imageGenerator.flow(x, batch_size=1, save_to_dir='/content/drive/My Drive/CTRC/train/3. Extra',\n",
        "                               save_prefix='{}'.format(file_list[j]), save_format='jpg'):\n",
        "\n",
        "        i += 1\n",
        "\n",
        "        if i > 5: \n",
        "\n",
        "            break"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Found 0 images belonging to 0 classes.\n",
            "Found 0 images belonging to 0 classes.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "F347m1zsDpIO",
        "colab_type": "text"
      },
      "source": [
        "4. Normal"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5Es_QupIDoyN",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "86890a86-5cc5-40dc-b441-ca1a904eb784"
      },
      "source": [
        "import os\n",
        "import numpy as np\n",
        "from PIL import Image\n",
        "#path = 이미지resize할 file directory\n",
        "path4 = \"/content/drive/My Drive/CTRC/validation/4. Normal\"\n",
        "\n",
        "file_list = os.listdir(path4)\n",
        "\n",
        "print (\"file_list: {}\".format(file_list))\n",
        "\n",
        "n=len(file_list)\n",
        "print(\"n=%d\"%(n))\n",
        "print(file_list[0])"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "file_list: ['nor_132.jpg', 'nor_133.jpg', 'nor_134.jpg', 'nor_135.jpg', 'nor_136.jpg', 'nor_137.jpg', 'nor_138.jpg', 'nor_139.jpg', 'nor_140.jpg', 'nor_141.jpg', 'nor_142.jpg', 'nor_143.jpg', 'nor_144.jpg', 'nor_145.jpg', 'nor_145_1.jpg', 'nor_146.jpg', 'nor_147.jpg', 'nor_148.jpg', 'nor_149.jpg', 'nor_149_1.jpg', 'nor_150.jpg', 'nor_151.jpg', 'nor_152.jpg', 'nor_152_1.jpg', 'nor_153.jpg', 'nor_154.jpg', 'nor_155.jpg', 'nor_156.jpg', 'nor_157.jpg', 'nor_158.jpg', 'nor_159.jpg', 'nor_160.jpg', 'nor_161.jpg', 'nor_162.jpg', 'nor_163.jpg', 'nor_164.jpg', 'nor_165.jpg', 'nor_166.jpg', 'nor_167.jpg', 'nor_168.jpg', 'nor_169.jpg', 'nor_171.jpg', 'nor_172.jpg', 'nor_173.jpg', 'nor_174.jpg', 'nor_175.jpg', 'nor_176.jpg', 'nor_177.jpg', 'nor_178.jpg', 'nor_179.jpg', 'nor_180.jpg', 'nor_181.jpg', 'nor_182.jpg', 'nor_183.jpg', 'nor_184.jpg', 'nor_185.jpg', 'nor_186.jpg', 'nor_187.jpg', 'nor_188.jpg', 'nor_189.jpg', 'nor_190.jpg', 'nor_191.jpg', 'nor_192.jpg', 'nor_193.jpg', 'nor_194.jpg', 'nor_195.jpg', 'nor_196.jpg', 'nor_197.jpg', 'nor_198.jpg', 'nor_199.jpg', 'nor_200.jpg', 'nor_201.jpg', 'nor_202.jpg', 'nor_203.jpg', 'nor_204.jpg', 'nor_318.jpg', 'nor_318_1.jpg', 'nor_318_2.jpg', 'nor_319.jpg', 'nor_319_1.jpg', 'nor_320.jpg', 'nor_320_1.jpg', 'nor_320_2.jpg', 'nor_321.jpg', 'nor_321_1.jpg', 'nor_321_2.jpg', 'nor_322.jpg', 'nor_322_1.jpg', 'nor_322_2.jpg', 'nor_323.jpg', 'nor_323_1.jpg', 'nor_323_2.jpg', 'nor_324.jpg', 'nor_324_1.jpg', 'nor_324_2.jpg', 'nor_324_3.jpg', 'nor_325.jpg', 'nor_325_1.jpg', 'nor_325_2.jpg', 'nor_325_3.jpg', 'nor_326_1.jpg', 'nor_328.jpg', 'nor_328_1.jpg', 'nor_328_2.jpg', 'nor_329.jpg', 'nor_329_1.jpg', 'nor_329_2.jpg', 'nor_330.jpg', 'nor_330_1.jpg', 'nor_330_2.jpg', 'nor_330_3.jpg', 'nor_331.jpg', 'nor_331_1.jpg', 'nor_331_2.jpg', 'nor_334.jpg', 'nor_334_1.jpg', 'nor_334_2.jpg', 'nor_335.jpg', 'nor_335_1.jpg', 'nor_336.jpg', 'nor_336_1.jpg', 'nor_336_2.jpg', 'nor_337.jpg', 'nor_337_1.jpg', 'nor_337_2.jpg', 'nor_338.jpg', 'nor_338_1.jpg', 'nor_339.jpg', 'nor_339_1.jpg', 'nor_340.jpg', 'nor_340_1.jpg', 'nor_340_2.jpg', 'nor_341.jpg', 'nor_341_1.jpg', 'nor_341_2.jpg', 'nor_341_3.jpg', 'nor_342.jpg', 'nor_342_1.jpg', 'nor_342_2.jpg', 'nor_343.jpg', 'nor_344.jpg', 'nor_344_1.jpg', 'nor_345.jpg', 'nor_346.jpg', 'nor_346_1.jpg', 'nor_346_2.jpg', 'nor_347.jpg', 'nor_348.jpg', 'nor_348_1.jpg', 'nor_348_2.jpg', 'nor_349.jpg', 'nor_349_1.jpg', 'nor_349_2.jpg', 'nor_350.jpg', 'nor_351.jpg', 'nor_351_1.jpg', 'nor_351_2.jpg', 'nor_352.jpg', 'nor_352_1.jpg', 'nor_352_2.jpg', 'nor_353.jpg', 'nor_353_1.jpg', 'nor_353_2.jpg', 'nor_354.jpg', 'nor_354_1.jpg', 'nor_354_2.jpg', 'nor_355.jpg', 'nor_355_1.jpg', 'nor_356.jpg', 'nor_356_1.jpg', 'nor_357.jpg', 'nor_358.jpg', 'nor_358_1.jpg', 'nor_359.jpg', 'nor_360.jpg', 'nor_361.jpg', 'nor_361_1.jpg', 'nor_362.jpg', 'nor_362_1.jpg', 'nor_363.jpg', 'nor_363_1.jpg', 'nor_363_2.jpg', 'nor_364.jpg', 'nor_366.jpg', 'nor_366_1.jpg', 'nor_367.jpg', 'nor_367_1.jpg', 'nor_368.jpg', 'nor_369.jpg', 'nor_372.jpg', 'nor_372_1.jpg', 'nor_373.jpg', 'nor_374.jpg', 'nor_374_1.jpg', 'nor_375.jpg', 'nor_375_1.jpg', 'nor_376.jpg', 'nor_377.jpg', 'nor_378.jpg', 'nor_379.jpg', 'nor_380.jpg', 'nor_380_1.jpg', 'nor_381.jpg', 'nor_382.jpg', 'nor_382_1.jpg', 'nor_382_2.jpg', 'nor_383.jpg', 'nor_383_1.jpg', 'nor_384.jpg', 'nor_384_1.jpg', 'nor_384_2.jpg', 'nor_385.jpg', 'nor_385_1.jpg', 'nor_385_2.jpg', 'nor_386.jpg', 'nor_386_1.jpg', 'nor_386_2.jpg', 'nor_387.jpg', 'nor_387_1.jpg', 'nor_388.jpg', 'nor_388_1.jpg', 'nor_389.jpg', 'nor_390.jpg', 'nor_390_1.jpg', 'nor_391.jpg', 'nor_391_1.jpg', 'nor_392.jpg', 'nor_392_1.jpg', 'nor_393.jpg', 'nor_393_1.jpg', 'nor_394.jpg', 'nor_394_1.jpg', 'nor_394_2.jpg', 'nor_395.jpg', 'nor_395_1.jpg', 'nor_395_2.jpg', 'nor_396.jpg', 'nor_396_1.jpg', 'nor_396_2.jpg', 'nor_397.jpg', 'nor_397_1.jpg', 'nor_397_2.jpg', 'nor_398.jpg', 'nor_398_1.jpg', 'nor_398_2.jpg', 'nor_399.jpg', 'nor_399_1.jpg', 'nor_400.jpg', 'nor_400_1.jpg', 'nor_401.jpg', 'nor_401_1.jpg', 'nor_401_2.jpg', 'nor_402.jpg', 'nor_403.jpg', 'nor_404.jpg', 'nor_405.jpg', 'nor_406.jpg', 'nor_406_1.jpg', 'nor_407.jpg', 'nor_407_1.jpg', 'nor_407_2.jpg', 'nor_408.jpg', 'nor_408_1.jpg', 'nor_409.jpg', 'nor_410.jpg', 'nor_410_1.jpg', 'nor_410_2.jpg', 'nor_411.jpg', 'nor_411_1.jpg', 'nor_411_2.jpg', 'nor_412.jpg', 'nor_412_1.jpg', 'nor_412_2.jpg', 'nor_413.jpg', 'nor_413_1.jpg', 'nor_413_2.jpg', 'nor_414.jpg', 'nor_414_1.jpg', 'nor_414_2.jpg', 'nor_415.jpg', 'nor_415_1.jpg', 'nor_415_2.jpg', 'nor_415_3.jpg', 'nor_416.jpg', 'nor_416_1.jpg', 'nor_416_2.jpg', 'nor_416_3.jpg', 'nor_417.jpg', 'nor_417_1.jpg', 'nor_417_2.jpg', 'nor_417_3.jpg', 'nor_417_4.jpg', 'nor_418.jpg', 'nor_418_1.jpg', 'nor_418_2.jpg', 'nor_419.jpg', 'nor_419_1.jpg', 'nor_419_2.jpg', 'nor_420.jpg', 'nor_420_1.jpg', 'nor_420_2.jpg', 'nor_420_3.jpg', 'nor_420_4.jpg', 'nor_421.jpg', 'nor_421_1.jpg', 'nor_421_2.jpg', 'nor_421_3.jpg', 'nor_421_4.jpg', 'nor_422.jpg', 'nor_422_1.jpg', 'nor_422_2.jpg', 'nor_422_3.jpg', 'nor_423.jpg', 'nor_423_1.jpg', 'nor_423_2.jpg', 'nor_423_3.jpg', 'nor_424.jpg', 'nor_424_1.jpg', 'nor_424_2.jpg', 'nor_425.jpg', 'nor_425_1.jpg', 'nor_425_2.jpg', 'nor_425_3.jpg', 'nor_426.jpg', 'nor_426_1.jpg', 'nor_426_2.jpg', 'nor_426_3.jpg', 'nor_427.jpg', 'nor_427_1.jpg', 'nor_427_2.jpg', 'nor_427_3.jpg', 'nor_428.jpg', 'nor_428_1.jpg', 'nor_428_2.jpg', 'nor_428_3.jpg', 'nor_429.jpg', 'nor_429_1.jpg', 'nor_429_2.jpg', 'nor_429_3.jpg', 'nor_430.jpg', 'nor_430_1.jpg', 'nor_430_2.jpg', 'nor_430_3.jpg', 'nor_431.jpg', 'nor_431_1.jpg', 'nor_431_2.jpg', 'nor_431_3.jpg', 'nor_434.jpg', 'nor_434_1.jpg', 'nor_435.jpg', 'nor_435_1.jpg', 'nor_436.jpg', 'nor_436_1.jpg', 'nor_436_2.jpg', 'nor_436_3.jpg', 'nor_437.jpg', 'nor_437_1.jpg', 'nor_437_2.jpg', 'nor_438.jpg', 'nor_438_1.jpg', 'nor_438_2.jpg', 'nor_439.jpg', 'nor_440.jpg', 'nor_441.jpg', 'nor_441_1.jpg', 'nor_441_2.jpg', 'nor_441_3.jpg', 'nor_442.jpg', 'nor_443.jpg', 'nor_444.jpg', 'nor_445.jpg', 'nor_446.jpg', 'nor_446_1.jpg', 'nor_446_2.jpg', 'nor_447.jpg', 'nor_448.jpg', 'nor_449.jpg', 'nor_450.jpg', 'nor_450_1.jpg', 'nor_450_2.jpg', 'nor_451.jpg', 'nor_452.jpg', 'nor_452_1.jpg', 'nor_452_2.jpg', 'nor_453.jpg', 'nor_453_1.jpg', 'nor_454.jpg', 'nor_454_1.jpg', 'nor_456.jpg', 'nor_457.jpg', 'nor_457_1.jpg', 'nor_458.jpg', 'nor_458_1.jpg', 'nor_455.jpg', 'nor_327.jpg', 'nor_332.jpg', 'nor_333.jpg', 'nor_83.jpg', 'nor_433.jpg', 'nor_317_2.jpg', 'nor_371.jpg', 'nor_432.jpg', 'nor_317_3.jpg', 'nor_370.jpg', 'nor_205_1.jpg', 'nor_206.jpg', 'nor_205_3.jpg', 'nor_206_2.jpg', 'nor_205_2.jpg', 'nor_205.jpg', 'nor_205_4.jpg', 'nor_206_1.jpg', 'nor_207_3.jpg', 'nor_207_4.jpg', 'nor_208_2.jpg', 'nor_207.jpg', 'nor_207_2.jpg', 'nor_208_3.jpg', 'nor_206_3.jpg', 'nor_208_1.jpg', 'nor_208.jpg', 'nor_206_4.jpg', 'nor_207_1.jpg', 'nor_209_4.jpg', 'nor_211.jpg', 'nor_210_1.jpg', 'nor_210_2.jpg', 'nor_209.jpg', 'nor_209_2.jpg', 'nor_209_3.jpg', 'nor_210_3.jpg', 'nor_210.jpg', 'nor_209_1.jpg', 'nor_210_4.jpg', 'nor_211_2.jpg', 'nor_211_1.jpg', 'nor_212.jpg', 'nor_211_3.jpg', 'nor_212_1.jpg', 'nor_212_2.jpg', 'nor_213_2.jpg', 'nor_212_4.jpg', 'nor_213_1.jpg', 'nor_213.jpg', 'nor_212_3.jpg', 'nor_213_3.jpg', 'nor_214_3.jpg', 'nor_214_4.jpg', 'nor_214_1.jpg', 'nor_215_1.jpg', 'nor_214.jpg', 'nor_215.jpg', 'nor_215_2.jpg', 'nor_214_2.jpg', 'nor_214_5.jpg', 'nor_216_4.jpg', 'nor_215_5.jpg', 'nor_216_1.jpg', 'nor_215_4.jpg', 'nor_216_2.jpg', 'nor_216.jpg', 'nor_215_3.jpg', 'nor_216_3.jpg', 'nor_217_7.jpg', 'nor_217_5.jpg', 'nor_217_2.jpg', 'nor_217_3.jpg', 'nor_217.jpg', 'nor_218_2.jpg', 'nor_218.jpg', 'nor_217_4.jpg', 'nor_217_1.jpg', 'nor_218_3.jpg', 'nor_217_6.jpg', 'nor_218_1.jpg', 'nor_219_3.jpg', 'nor_218_5.jpg', 'nor_219_4.jpg', 'nor_219.jpg', 'nor_218_4.jpg', 'nor_218_6.jpg', 'nor_219_1.jpg', 'nor_219_2.jpg', 'nor_220_2.jpg', 'nor_220_5.jpg', 'nor_220_1.jpg', 'nor_220_3.jpg', 'nor_220.jpg', 'nor_220_4.jpg', 'nor_221_4.jpg', 'nor_221_3.jpg', 'nor_221_2.jpg', 'nor_222.jpg', 'nor_222_2.jpg', 'nor_221.jpg', 'nor_221_1.jpg', 'nor_222_1.jpg', 'nor_222_4.jpg', 'nor_222_5.jpg', 'nor_223_1.jpg', 'nor_223_2.jpg', 'nor_223_3.jpg', 'nor_222_3.jpg', 'nor_223.jpg', 'nor_224_3.jpg', 'nor_223_4.jpg', 'nor_224_2.jpg', 'nor_224_1.jpg', 'nor_224_4.jpg', 'nor_224.jpg', 'nor_225.jpg', 'nor_225_4.jpg', 'nor_226.jpg', 'nor_225_2.jpg', 'nor_225_3.jpg', 'nor_226_1.jpg', 'nor_225_1.jpg', 'nor_227_3.jpg', 'nor_226_4.jpg', 'nor_226_2.jpg', 'nor_227_1.jpg', 'nor_226_3.jpg', 'nor_227_2.jpg', 'nor_227.jpg', 'nor_227_4.jpg', 'nor_228_3.jpg', 'nor_228_4.jpg', 'nor_228_1.jpg', 'nor_227_5.jpg', 'nor_228.jpg', 'nor_228_2.jpg', 'nor_229_3.jpg', 'nor_229_6.jpg', 'nor_229_1.jpg', 'nor_229_2.jpg', 'nor_229_4.jpg', 'nor_229.jpg', 'nor_230.jpg', 'nor_229_5.jpg', 'nor_231_2.jpg', 'nor_230_2.jpg', 'nor_230_4.jpg', 'nor_230_1.jpg', 'nor_231.jpg', 'nor_231_3.jpg', 'nor_230_3.jpg', 'nor_230_5.jpg', 'nor_231_4.jpg', 'nor_231_1.jpg', 'nor_232.jpg', 'nor_233.jpg', 'nor_232_2.jpg', 'nor_233_2.jpg', 'nor_232_4.jpg', 'nor_232_5.jpg', 'nor_233_3.jpg', 'nor_233_1.jpg', 'nor_232_1.jpg', 'nor_232_3.jpg', 'nor_231_6.jpg', 'nor_231_5.jpg', 'nor_233_4.jpg', 'nor_234_2.jpg', 'nor_234_4.jpg', 'nor_234.jpg', 'nor_234_1.jpg', 'nor_234_3.jpg', 'nor_233_5.jpg', 'nor_233_6.jpg', 'nor_235_2.jpg', 'nor_235_1.jpg', 'nor_234_6.jpg', 'nor_235.jpg', 'nor_234_7.jpg', 'nor_235_3.jpg', 'nor_234_5.jpg', 'nor_237_1.jpg', 'nor_237_2.jpg', 'nor_235_4.jpg', 'nor_236_3.jpg', 'nor_237.jpg', 'nor_235_5.jpg', 'nor_235_6.jpg', 'nor_236_2.jpg', 'nor_236.jpg', 'nor_236_1.jpg', 'nor_238_2.jpg', 'nor_237_4.jpg', 'nor_238_1.jpg', 'nor_237_3.jpg', 'nor_237_5.jpg', 'nor_238_3.jpg', 'nor_238.jpg', 'nor_239_2.jpg', 'nor_239_3.jpg', 'nor_239_5.jpg', 'nor_238_4.jpg', 'nor_239_4.jpg', 'nor_239_6.jpg', 'nor_239.jpg', 'nor_239_1.jpg', 'nor_240_1.jpg', 'nor_240_4.jpg', 'nor_241_1.jpg', 'nor_240.jpg', 'nor_240_5.jpg', 'nor_240_2.jpg', 'nor_241.jpg', 'nor_241_2.jpg', 'nor_240_3.jpg', 'nor_241_5.jpg', 'nor_242_3.jpg', 'nor_242_1.jpg', 'nor_242_4.jpg', 'nor_242_2.jpg', 'nor_241_3.jpg', 'nor_241_4.jpg', 'nor_242.jpg', 'nor_241_7.jpg', 'nor_241_6.jpg', 'nor_244_2.jpg', 'nor_244.jpg', 'nor_243.jpg', 'nor_243_4.jpg', 'nor_243_2.jpg', 'nor_243_1.jpg', 'nor_244_3.jpg', 'nor_243_5.jpg', 'nor_243_3.jpg', 'nor_242_5.jpg', 'nor_244_1.jpg', 'nor_246_5.jpg', 'nor_245_4.jpg', 'nor_244_5.jpg', 'nor_245.jpg', 'nor_246.jpg', 'nor_245_2.jpg', 'nor_246_2.jpg', 'nor_245_1.jpg', 'nor_246_3.jpg', 'nor_245_3.jpg', 'nor_246_4.jpg', 'nor_246_1.jpg', 'nor_244_4.jpg', 'nor_247_1.jpg', 'nor_247_5.jpg', 'nor_249_1.jpg', 'nor_249.jpg', 'nor_248_3.jpg', 'nor_248_2.jpg', 'nor_247_2.jpg', 'nor_247_4.jpg', 'nor_248_1.jpg', 'nor_248.jpg', 'nor_247_3.jpg', 'nor_247.jpg', 'nor_250_5.jpg', 'nor_250_1.jpg', 'nor_251_3.jpg', 'nor_251_1.jpg', 'nor_251_2.jpg', 'nor_250.jpg', 'nor_250_6.jpg', 'nor_249_2.jpg', 'nor_251.jpg', 'nor_250_4.jpg', 'nor_250_2.jpg', 'nor_250_3.jpg', 'nor_249_3.jpg', 'nor_252.jpg', 'nor_251_5.jpg', 'nor_253.jpg', 'nor_253_2.jpg', 'nor_252_5.jpg', 'nor_253_4.jpg', 'nor_253_1.jpg', 'nor_252_2.jpg', 'nor_252_1.jpg', 'nor_252_4.jpg', 'nor_252_3.jpg', 'nor_254_5.jpg', 'nor_254_1.jpg', 'nor_254.jpg', 'nor_255_3.jpg', 'nor_255.jpg', 'nor_255_2.jpg', 'nor_255_4.jpg', 'nor_255_1.jpg', 'nor_256.jpg', 'nor_254_2.jpg', 'nor_256_1.jpg', 'nor_254_3.jpg', 'nor_254_4.jpg', 'nor_257_5.jpg', 'nor_256_3.jpg', 'nor_257_1.jpg', 'nor_256_5.jpg', 'nor_257_2.jpg', 'nor_256_6.jpg', 'nor_256_4.jpg', 'nor_257_4.jpg', 'nor_257.jpg', 'nor_256_2.jpg', 'nor_257_3.jpg', 'nor_258_5.jpg', 'nor_258_4.jpg', 'nor_259_2.jpg', 'nor_257_6.jpg', 'nor_258.jpg', 'nor_258_1.jpg', 'nor_258_2.jpg', 'nor_260.jpg', 'nor_258_6.jpg', 'nor_259_3.jpg', 'nor_259_4.jpg', 'nor_258_3.jpg', 'nor_259.jpg', 'nor_259_1.jpg', 'nor_262_4.jpg', 'nor_260_2.jpg', 'nor_261_1.jpg', 'nor_262.jpg', 'nor_262_1.jpg', 'nor_261_2.jpg', 'nor_262_2.jpg', 'nor_260_3.jpg', 'nor_260_1.jpg', 'nor_261_3.jpg', 'nor_262_3.jpg', 'nor_261.jpg', 'nor_262_6.jpg', 'nor_264_3.jpg', 'nor_263.jpg', 'nor_263_3.jpg', 'nor_263_2.jpg', 'nor_264.jpg', 'nor_264_2.jpg', 'nor_263_1.jpg', 'nor_264_1.jpg', 'nor_262_5.jpg', 'nor_264_4.jpg', 'nor_265_5.jpg', 'nor_265.jpg', 'nor_265_2.jpg', 'nor_265_1.jpg', 'nor_265_3.jpg', 'nor_266_2.jpg', 'nor_265_4.jpg', 'nor_266.jpg', 'nor_266_1.jpg', 'nor_267_2.jpg', 'nor_266_4.jpg', 'nor_267_4.jpg', 'nor_267_1.jpg', 'nor_267_3.jpg', 'nor_267.jpg', 'nor_266_3.jpg', 'nor_268_2.jpg', 'nor_268_1.jpg', 'nor_267_5.jpg', 'nor_268.jpg', 'nor_269_2.jpg', 'nor_269_4.jpg', 'nor_270_3.jpg', 'nor_269_3.jpg', 'nor_270_1.jpg', 'nor_269_1.jpg', 'nor_270.jpg', 'nor_269.jpg', 'nor_270_4.jpg', 'nor_268_3.jpg', 'nor_270_2.jpg', 'nor_271.jpg', 'nor_272_3.jpg', 'nor_271_5.jpg', 'nor_272_2.jpg', 'nor_271_4.jpg', 'nor_271_2.jpg', 'nor_272_1.jpg', 'nor_271_3.jpg', 'nor_271_1.jpg', 'nor_272.jpg', 'nor_274_1.jpg', 'nor_274_2.jpg', 'nor_274_4.jpg', 'nor_272_4.jpg', 'nor_274.jpg', 'nor_275_1.jpg', 'nor_274_3.jpg', 'nor_273.jpg', 'nor_273_1.jpg', 'nor_275.jpg', 'nor_275_2.jpg', 'nor_278.jpg', 'nor_276_2.jpg', 'nor_277_1.jpg', 'nor_276_1.jpg', 'nor_277.jpg', 'nor_276_5.jpg', 'nor_278_1.jpg', 'nor_276.jpg', 'nor_276_4.jpg', 'nor_276_3.jpg', 'nor_279.jpg', 'nor_278_2.jpg', 'nor_280_1.jpg', 'nor_279_2.jpg', 'nor_278_3.jpg', 'nor_279_3.jpg', 'nor_280.jpg', 'nor_279_1.jpg', 'nor_280_3.jpg', 'nor_280_4.jpg', 'nor_280_2.jpg', 'nor_283_2.jpg', 'nor_282.jpg', 'nor_282_2.jpg', 'nor_283.jpg', 'nor_282_1.jpg', 'nor_282_3.jpg', 'nor_281_1.jpg', 'nor_281_2.jpg', 'nor_284.jpg', 'nor_283_1.jpg', 'nor_281_3.jpg', 'nor_284_1.jpg', 'nor_281.jpg', 'nor_286_1.jpg', 'nor_284_2.jpg', 'nor_285_5.jpg', 'nor_285_4.jpg', 'nor_285_3.jpg', 'nor_285_2.jpg', 'nor_286_3.jpg', 'nor_286.jpg', 'nor_285_1.jpg', 'nor_284_3.jpg', 'nor_285.jpg', 'nor_286_2.jpg', 'nor_287.jpg', 'nor_286_4.jpg', 'nor_287_1.jpg', 'nor_288.jpg', 'nor_288_1.jpg', 'nor_287_2.jpg', 'nor_288_2.jpg', 'nor_288_3.jpg', 'nor_287_3.jpg', 'nor_290_1.jpg', 'nor_290_3.jpg', 'nor_289_4.jpg', 'nor_289.jpg', 'nor_291.jpg', 'nor_289_2.jpg', 'nor_290_2.jpg', 'nor_289_1.jpg', 'nor_290_4.jpg', 'nor_289_3.jpg', 'nor_290.jpg', 'nor_289_5.jpg', 'nor_291_4.jpg', 'nor_292_2.jpg', 'nor_292_3.jpg', 'nor_292_1.jpg', 'nor_291_3.jpg', 'nor_291_1.jpg', 'nor_292.jpg', 'nor_292_4.jpg', 'nor_291_2.jpg', 'nor_293.jpg', 'nor_295.jpg', 'nor_293_2.jpg', 'nor_294_3.jpg', 'nor_295_2.jpg', 'nor_293_3.jpg', 'nor_294_2.jpg', 'nor_293_4.jpg', 'nor_293_1.jpg', 'nor_294_1.jpg', 'nor_295_1.jpg', 'nor_294.jpg', 'nor_296_2.jpg', 'nor_297_2.jpg', 'nor_296_3.jpg', 'nor_296.jpg', 'nor_296_4.jpg', 'nor_295_4.jpg', 'nor_296_1.jpg', 'nor_295_3.jpg', 'nor_297_1.jpg', 'nor_297.jpg', 'nor_297_4.jpg', 'nor_297_3.jpg', 'nor_298.jpg', 'nor_298_3.jpg', 'nor_298_4.jpg', 'nor_299.jpg', 'nor_299_1.jpg', 'nor_298_1.jpg', 'nor_299_3.jpg', 'nor_299_2.jpg', 'nor_298_2.jpg', 'nor_299_4.jpg', 'nor_300.jpg', 'nor_300_3.jpg', 'nor_301_2.jpg', 'nor_301_1.jpg', 'nor_300_2.jpg', 'nor_301.jpg', 'nor_300_4.jpg', 'nor_300_1.jpg', 'nor_301_4.jpg', 'nor_301_5.jpg', 'nor_303_3.jpg', 'nor_302.jpg', 'nor_303_2.jpg', 'nor_302_3.jpg', 'nor_302_1.jpg', 'nor_302_2.jpg', 'nor_304.jpg', 'nor_303_1.jpg', 'nor_303.jpg', 'nor_301_3.jpg', 'nor_304_2.jpg', 'nor_305_4.jpg', 'nor_305_1.jpg', 'nor_304_3.jpg', 'nor_305.jpg', 'nor_305_3.jpg', 'nor_306_1.jpg', 'nor_304_4.jpg', 'nor_304_1.jpg', 'nor_305_5.jpg', 'nor_306.jpg', 'nor_305_2.jpg', 'nor_307_1.jpg', 'nor_306_2.jpg', 'nor_306_4.jpg', 'nor_308_3.jpg', 'nor_307_3.jpg', 'nor_307.jpg', 'nor_308_1.jpg', 'nor_308_2.jpg', 'nor_306_3.jpg', 'nor_307_4.jpg', 'nor_307_2.jpg', 'nor_308.jpg', 'nor_311_2.jpg', 'nor_309_2.jpg', 'nor_310_3.jpg', 'nor_309.jpg', 'nor_309_4.jpg', 'nor_311.jpg', 'nor_311_1.jpg', 'nor_309_3.jpg', 'nor_310_2.jpg', 'nor_310_1.jpg', 'nor_310.jpg', 'nor_309_1.jpg', 'nor_312.jpg', 'nor_313_2.jpg', 'nor_312_2.jpg', 'nor_313_1.jpg', 'nor_313.jpg', 'nor_311_4.jpg', 'nor_312_3.jpg', 'nor_311_3.jpg', 'nor_312_1.jpg', 'nor_313_3.jpg', 'nor_312_4.jpg', 'nor_314_2.jpg', 'nor_314_4.jpg', 'nor_313_4.jpg', 'nor_315.jpg', 'nor_314_1.jpg', 'nor_315_1.jpg', 'nor_314_3.jpg', 'nor_313_6.jpg', 'nor_314.jpg', 'nor_313_5.jpg', 'nor_316_1.jpg', 'nor_315_2.jpg', 'nor_317_1.jpg', 'nor_316_3.jpg', 'nor_317.jpg', 'nor_315_3.jpg', 'nor_316_2.jpg', 'nor_316_4.jpg', 'nor_316.jpg', 'nor_326.jpg', 'nor_326_2.jpg', 'nor_365.jpg', 'nor_54.jpg', 'nor_55.jpg', 'nor_73.jpg', 'nor_170.jpg', 'nor_251_4.jpg', 'nor_253_3.jpg', 'nor_263_4.jpg', 'nor_263_5.jpg', 'nor_3.jpg', 'nor_1.jpg', 'nor_2.jpg', 'nor_4.jpg', 'nor_5.jpg', 'nor_6.jpg', 'nor_7.jpg', 'nor_8.jpg', 'nor_9.jpg', 'nor_10.jpg', 'nor_11.jpg', 'nor_12.jpg', 'nor_13.jpg', 'nor_14.jpg', 'nor_15.jpg', 'nor_16.jpg', 'nor_17.jpg', 'nor_18.jpg', 'nor_19.jpg', 'nor_20.jpg', 'nor_21.jpg', 'nor_23.jpg', 'nor_24.jpg', 'nor_25.jpg', 'nor_26.jpg', 'nor_27.jpg', 'nor_28.jpg', 'nor_29.jpg', 'nor_22.jpg', 'nor_30.jpg', 'nor_31.jpg', 'nor_32.jpg', 'nor_33.jpg', 'nor_34.jpg', 'nor_35.jpg', 'nor_36.jpg', 'nor_37.jpg', 'nor_38.jpg', 'nor_39.jpg', 'nor_40.jpg', 'nor_41.jpg', 'nor_42.jpg', 'nor_42_1.jpg', 'nor_43.jpg', 'nor_44.jpg', 'nor_45.jpg', 'nor_46.jpg', 'nor_47.jpg', 'nor_48.jpg', 'nor_49.jpg', 'nor_50.jpg', 'nor_51.jpg', 'nor_52.jpg', 'nor_53.jpg', 'nor_56.jpg', 'nor_57.jpg', 'nor_58.jpg', 'nor_59.jpg', 'nor_60.jpg', 'nor_61.jpg', 'nor_62.jpg', 'nor_63.jpg', 'nor_64.jpg', 'nor_65.jpg', 'nor_66.jpg', 'nor_67.jpg', 'nor_68.jpg', 'nor_69.jpg', 'nor_70.jpg', 'nor_71.jpg', 'nor_72.jpg', 'nor_74.jpg', 'nor_75.jpg', 'nor_76.jpg', 'nor_77.jpg', 'nor_78.jpg', 'nor_78_1.jpg', 'nor_79.jpg', 'nor_80.jpg', 'nor_81.jpg', 'nor_82.jpg', 'nor_84.jpg', 'nor_85.jpg', 'nor_86.jpg', 'nor_87.jpg', 'nor_88.jpg', 'nor_89.jpg', 'nor_89_1.jpg', 'nor_90.jpg', 'nor_90_1.jpg', 'nor_91.jpg', 'nor_91_1.jpg', 'nor_92.jpg', 'nor_93.jpg', 'nor_94.jpg', 'nor_94_1.jpg', 'nor_95.jpg', 'nor_95_1.jpg', 'nor_96.jpg', 'nor_97.jpg', 'nor_97_1.jpg', 'nor_98.jpg', 'nor_99.jpg', 'nor_100.jpg', 'nor_101.jpg', 'nor_102.jpg', 'nor_103.jpg', 'nor_104.jpg', 'nor_105.jpg', 'nor_106.jpg', 'nor_107.jpg', 'nor_108.jpg', 'nor_109.jpg', 'nor_110.jpg', 'nor_111.jpg', 'nor_112.jpg', 'nor_113.jpg', 'nor_114.jpg', 'nor_114_1.jpg', 'nor_115.jpg', 'nor_116.jpg', 'nor_117.jpg', 'nor_118.jpg', 'nor_119.jpg', 'nor_120.jpg', 'nor_121.jpg', 'nor_122.jpg', 'nor_122_1.jpg', 'nor_123.jpg', 'nor_124.jpg', 'nor_125.jpg', 'nor_126.jpg', 'nor_127.jpg', 'nor_128.jpg', 'nor_129.jpg', 'nor_130.jpg', 'nor_131.jpg']\n",
            "n=1137\n",
            "nor_132.jpg\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "o9Y99gTdDvQY",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "8812b435-250f-412d-fef5-b40d8b2ecb01"
      },
      "source": [
        "np.random.seed(5)\n",
        "from keras.preprocessing.image import ImageDataGenerator, array_to_img, img_to_array, load_img\n",
        "\n",
        "\n",
        "#데이터가 충분하면 나누기\n",
        "trainGen = imageGenerator.flow_from_directory('/content/drive/My Drive/CTRC/train/4. Normal',\n",
        "                                              target_size=(64,64),\n",
        "                                              subset='training')\n",
        "\n",
        "validationGen = imageGenerator.flow_from_directory('/content/drive/My Drive/CTRC/validation/4. Normal',\n",
        "                                                  target_size=(64,64),\n",
        "                                                  subset='validation')\n",
        "\n",
        "\n",
        "\n",
        "for j in range(n):\n",
        "    \n",
        "    img = load_img('/content/drive/My Drive/CTRC/validation/4. Normal/{}'.format(file_list[j]))\n",
        "    x = img_to_array(img)\n",
        "    x = x.reshape((1,) + x.shape)\n",
        "\n",
        "    i = 0\n",
        "\n",
        "    for batch in imageGenerator.flow(x, batch_size=1, save_to_dir='/content/drive/My Drive/CTRC/train/4. Normal',\n",
        "                               save_prefix='{}'.format(file_list[j]), save_format='jpg'):\n",
        "\n",
        "        i += 1\n",
        "\n",
        "        if i > 5: \n",
        "\n",
        "            break"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Found 0 images belonging to 0 classes.\n",
            "Found 0 images belonging to 0 classes.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3ArNio6RRgTe",
        "colab_type": "text"
      },
      "source": [
        "### 폴더 내 이미지 개수 -**train**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8XvzMI44RjGh",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "b2b30b41-ff45-42e5-f873-460b32bea504"
      },
      "source": [
        "#코드 추가해서 확인하기\n",
        "\n",
        "import os\n",
        "\n",
        "path, dirs, files = next(os.walk(\"/content/drive/My Drive/CTRC/train/4. Normal\"))\n",
        "file_count = len(files)\n",
        "\n",
        "file_count"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "6821"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 19
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "a7mC-hEeTsAw",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "7041d102-3779-43bc-c519-8da790ee5c67"
      },
      "source": [
        "import os\n",
        "\n",
        "path, dirs, files = next(os.walk(\"/content/drive/My Drive/CTRC/train/3. Extra\"))\n",
        "file_count = len(files)\n",
        "\n",
        "file_count"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "1571"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 20
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "W29rppybTr-8",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "56d9e8f7-604a-45c9-c13b-a1abd660dd2d"
      },
      "source": [
        "import os\n",
        "\n",
        "path, dirs, files = next(os.walk(\"/content/drive/My Drive/CTRC/train/2. Precancer\"))\n",
        "file_count = len(files)\n",
        "\n",
        "file_count"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "899"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 21
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "U_eyxexgT4V9",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "8cab8cf0-4cc4-45bf-e592-340480a3c331"
      },
      "source": [
        "import os\n",
        "\n",
        "path, dirs, files = next(os.walk(\"/content/drive/My Drive/CTRC/train/1. Cancer\"))\n",
        "file_count = len(files)\n",
        "\n",
        "file_count"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "2471"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 22
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "V5_HvWmEda9r",
        "colab_type": "text"
      },
      "source": [
        "## 설계\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NpIHpYP_dwBs",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "categories = [\"1. Cancer\",\"2. Precancer\",\"3. Extra\",\"4. Normal\"]\n",
        "nb_classes = len(categories)\n"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YZdm7Q0ZdZZy",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 690
        },
        "outputId": "47794322-9a9e-458c-db35-0eb7471f257a"
      },
      "source": [
        "from PIL import Image\n",
        "import os, glob, numpy as np\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "caltech_dir = '/content/drive/My Drive/CTRC/train'\n",
        "categories = [\"1. Cancer\",\"2. Precancer\",\"3. Extra\",\"4. Normal\"]\n",
        "nb_classes = len(categories)\n",
        "\n",
        "image_w = 64\n",
        "image_h = 64\n",
        "\n",
        "pixels = image_h * image_w * 3\n",
        "\n",
        "X = []\n",
        "y = []\n",
        "\n",
        "for idx, cat in enumerate(categories):\n",
        "    \n",
        "    #one-hot 돌리기.\n",
        "    label = [0 for i in range(nb_classes)]\n",
        "    label[idx] = 1\n",
        "\n",
        "    image_dir = caltech_dir + \"/\" + cat\n",
        "    files = glob.glob(image_dir+\"/*.jpg\")\n",
        "    print(cat, \" 파일 길이 : \", len(files))\n",
        "    for i, f in enumerate(files):\n",
        "        img = Image.open(f)\n",
        "        img = img.convert(\"RGB\")\n",
        "        img = img.resize((image_w, image_h))\n",
        "        data = np.asarray(img)\n",
        "\n",
        "        X.append(data)\n",
        "        y.append(label)\n",
        "\n",
        "        if i % 700 == 0:\n",
        "            print(cat, \" : \", f)\n",
        "\n",
        "X = np.array(X)\n",
        "y = np.array(y)\n",
        "#1 0 0 0 이면 airplanes\n",
        "#0 1 0 0 이면 buddha 이런식\n",
        "\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=321)\n",
        "xy = (X_train, X_test, y_train, y_test)\n",
        "np.save('/content/drive/My Drive/CTRC/train/multi_image_data.npy', xy)\n",
        "\n",
        "print(\"ok\", len(y))\n"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "1. Cancer  파일 길이 :  2471\n",
            "1. Cancer  :  /content/drive/My Drive/CTRC/train/1. Cancer/can_232.jpg_0_9397.jpg\n",
            "1. Cancer  :  /content/drive/My Drive/CTRC/train/1. Cancer/can_349.jpg_0_240.jpg\n",
            "1. Cancer  :  /content/drive/My Drive/CTRC/train/1. Cancer/can_132.jpg_0_9914.jpg\n",
            "1. Cancer  :  /content/drive/My Drive/CTRC/train/1. Cancer/can_17.jpg_0_4612.jpg\n",
            "2. Precancer  파일 길이 :  899\n",
            "2. Precancer  :  /content/drive/My Drive/CTRC/train/2. Precancer/precan_1.jpg_0_2459.jpg\n",
            "2. Precancer  :  /content/drive/My Drive/CTRC/train/2. Precancer/precan_114.jpg_0_6754.jpg\n",
            "3. Extra  파일 길이 :  1571\n",
            "3. Extra  :  /content/drive/My Drive/CTRC/train/3. Extra/inf_86.jpg_0_9860.jpg\n",
            "3. Extra  :  /content/drive/My Drive/CTRC/train/3. Extra/inf_201.jpg_0_5105.jpg\n",
            "3. Extra  :  /content/drive/My Drive/CTRC/train/3. Extra/inf_57.jpg_0_8027.jpg\n",
            "4. Normal  파일 길이 :  6821\n",
            "4. Normal  :  /content/drive/My Drive/CTRC/train/4. Normal/nor_314_2.jpg_0_1991.jpg\n",
            "4. Normal  :  /content/drive/My Drive/CTRC/train/4. Normal/nor_89_1.jpg_0_2121.jpg\n",
            "4. Normal  :  /content/drive/My Drive/CTRC/train/4. Normal/nor_295.jpg_0_412.jpg\n",
            "4. Normal  :  /content/drive/My Drive/CTRC/train/4. Normal/nor_248.jpg_0_4757.jpg\n",
            "4. Normal  :  /content/drive/My Drive/CTRC/train/4. Normal/nor_268_3.jpg_0_5038.jpg\n",
            "4. Normal  :  /content/drive/My Drive/CTRC/train/4. Normal/nor_232_4.jpg_0_4848.jpg\n",
            "4. Normal  :  /content/drive/My Drive/CTRC/train/4. Normal/nor_429_2.jpg_0_3941.jpg\n",
            "4. Normal  :  /content/drive/My Drive/CTRC/train/4. Normal/nor_214_2.jpg_0_5897.jpg\n",
            "4. Normal  :  /content/drive/My Drive/CTRC/train/4. Normal/nor_396_1.jpg_0_7886.jpg\n",
            "4. Normal  :  /content/drive/My Drive/CTRC/train/4. Normal/nor_180.jpg_0_3230.jpg\n",
            "ok 11762\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pmPn9lZpdhMK",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 214
        },
        "outputId": "68381447-058c-4d5b-e169-202fdb376a20"
      },
      "source": [
        "import os, glob, numpy as np\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Conv2D, MaxPooling2D, Dense, Flatten, Dropout\n",
        "from keras.callbacks import EarlyStopping, ModelCheckpoint\n",
        "import matplotlib.pyplot as plt\n",
        "import keras.backend.tensorflow_backend as K\n",
        "\n",
        "import tensorflow as tf\n",
        "config = tf.compat.v1.ConfigProto()\n",
        "config.gpu_options.allow_growth = True\n",
        "session = tf.compat.v1.Session(config=config)\n",
        "\n",
        "X_train, X_test, y_train, y_test = np.load('/content/drive/My Drive/CTRC/train/multi_image_data.npy', \n",
        "                                           allow_pickle=True)\n",
        "\n",
        "\n",
        "#일반화\n",
        "X_train = X_train.astype(float) / 255\n",
        "X_test = X_test.astype(float) / 255\n",
        "\n",
        "print(len(y_train))\n",
        "print(len(X_train))\n",
        "print(len(X_test))\n",
        "print(len(y_test))\n",
        "print(X_train.shape)\n",
        "print(X_train.shape[0])"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Using TensorFlow backend.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "8233\n",
            "8233\n",
            "3529\n",
            "3529\n",
            "(8233, 64, 64, 3)\n",
            "8233\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9352Bt6Inuk7",
        "colab_type": "text"
      },
      "source": [
        "## 2번 CNN 모델 만들기 (Conv2D 이용)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mhc9w51LVj7s",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 270
        },
        "outputId": "59f7d3a1-7aec-44dd-9d12-943991a27ab2"
      },
      "source": [
        "#import keras.backend.tensorflow_backend as K\n",
        "import tensorflow.keras.backend as K\n",
        "import os, glob, numpy as np\n",
        "\n",
        "import tensorflow.compat.v1 as tf\n",
        "tf.disable_v2_behavior()\n",
        "\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Conv2D, MaxPooling2D, Dense, Flatten, Dropout\n",
        "from keras.callbacks import EarlyStopping, ModelCheckpoint\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "\n",
        "\n",
        "model = Sequential()\n",
        "model.add(Conv2D(32, (7,7), padding=\"same\", input_shape=(64,64,3), activation='relu'))\n",
        "model.add(MaxPooling2D(pool_size=(2,2)))\n",
        "    #model.add(Dropout(0.25))\n",
        "    # 3X3 크기의 컨볼루션 레이어를 32개의 필터수를 처음에 생성\n",
        "    # 활성화 함수 relu\n",
        "    # (64,64,3)의 튜플 값 가진다\n",
        "    # Maxpooling2D를 통해 중요 값만 뽑아 작은 출력값 만든다\n",
        "\n",
        "    \n",
        "model.add(Conv2D(64, (5,5), padding=\"same\", activation='relu'))\n",
        "model.add(MaxPooling2D(pool_size=(2,2)))\n",
        "#model.add(Dropout(0.1))\n",
        "\n",
        "model.add(Conv2D(64, (3,3), padding=\"same\", activation='relu'))\n",
        "model.add(MaxPooling2D(pool_size=(2,2)))\n",
        "#model.add(Dropout(0.1))\n",
        "\n",
        "model.add(Conv2D(128, (3,3), padding=\"same\", activation='relu'))\n",
        "model.add(MaxPooling2D(pool_size=(2,2)))\n",
        "#model.add(Dropout(0.1))\n",
        "\n",
        "\n",
        "model.add(Conv2D(128, (3,3), padding=\"same\", activation='relu'))\n",
        "model.add(MaxPooling2D(pool_size=(2,2)))\n",
        "#odel.add(Dropout(0.1))\n",
        "\n",
        "\n",
        "model.add(Conv2D(256, (3,3), padding=\"same\", activation='relu'))\n",
        "model.add(MaxPooling2D(pool_size=(2,2)))\n",
        "\n",
        "\n",
        "model.add(Conv2D(256, (3,3), padding=\"same\", activation='relu'))\n",
        "model.add(MaxPooling2D(pool_size=(1,1)))\n",
        "model.add(Dropout(0.1))\n",
        "\n",
        "model.add(Conv2D(512, (3,3), padding=\"same\", activation='relu'))\n",
        "model.add(MaxPooling2D(pool_size=(1,1)))\n",
        "model.add(Dropout(0.1))\n",
        "\n",
        "\n",
        "model.add(Flatten()) \n",
        "    #CNN에서 컨볼루션 레이어나 맥스풀링을 거치면 주요 특징만 추출되어 학습됨\n",
        "    \n",
        "    #컨볼루션이나 맥스풀링은 주로 2차원을 다루지만\n",
        "    #전결합층에 전달을 하기 위해서는 1차원으로 바꿔야하는데\n",
        "    #이 때 Flatten사용\n",
        "model.add(Dense(256, activation='relu'))\n",
        "#model.add(Dropout(0.25))\n",
        "\n",
        "model.add(Dense(64, activation='relu'))\n",
        "#model.add(Dropout(0.25))\n",
        "\n",
        "model.add(Dense(32, activation='relu'))\n",
        "#model.add(Dropout(0.25))\n",
        "\n",
        "model.add(Dense(16, activation='relu'))\n",
        "#model.add(Dropout(0.25))\n",
        "\n",
        "model.add(Dense(16, activation='relu'))\n",
        "#model.add(Dropout(0.25))\n",
        "\n",
        "model.add(Dense(8, activation='relu'))\n",
        "model.add(Dropout(0.1))\n",
        "\n",
        "model.add(Dense(8, activation='relu'))\n",
        "model.add(Dropout(0.1))\n",
        "\n",
        "    #model.add(Dropout(0.5))\n",
        "\n",
        "model.add(Dense(4, activation='softmax'))\n",
        "\n",
        "\n",
        "model.compile(loss='kullback_leibler_divergence', optimizer='adam', metrics=['accuracy'])\n",
        "model_dir = './model'\n",
        "    \n",
        "if not os.path.exists(model_dir):\n",
        "    os.mkdir(model_dir)\n",
        "    \n",
        "model_path = model_dir + '/multi_img_classification.model'\n",
        "checkpoint = ModelCheckpoint(filepath=model_path , monitor='val_loss', verbose=1, save_best_only=True)\n",
        "early_stopping = EarlyStopping(monitor='val_loss', patience=6)"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow/python/compat/compat.py:175: disable_resource_variables (from tensorflow.python.ops.variable_scope) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "non-resource variables are not supported in the long term\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/op_def_library.py:263: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Colocations handled automatically by placer.\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:3445: calling dropout (from tensorflow.python.ops.nn_ops) with keep_prob is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Please use `rate` instead of `keep_prob`. Rate should be set to `rate = 1 - keep_prob`.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ATW202c9Q6Ct",
        "colab_type": "text"
      },
      "source": [
        "\n",
        "```\n",
        "model.compile(optimizer='adam',\n",
        "              loss='categorical_crossentropy',\n",
        "              metrics=['accuracy'])# 코드로 형식 지정됨\n",
        "```\n",
        "\n",
        "\n",
        "-손실 함수(Loss function)-훈련 하는 동안 모델의 오차를 측정합니다. 모델의 학습이 올바른 방향으로 향하도록 이 함수를 최소화해야 합니다.\n",
        "\n",
        "-옵티마이저(Optimizer)-데이터와 손실 함수를 바탕으로 모델의 업데이트 방법을 결정합니다.\n",
        "\n",
        "-지표(Metrics)-훈련 단계와 테스트 단계를 모니터링하기 위해 사용합니다. 다음 예에서는 올바르게 분류된 이미지의 비율인 정확도를 사용합니다."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7LISq6dNrZJs",
        "colab_type": "text"
      },
      "source": [
        "\n",
        "\n",
        "```\n",
        "model.add(Dropout(0.25))\n",
        "```\n",
        "\n",
        "\n",
        "오버피팅을 피하기 위하여 25% 드롭아웃\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "c6RTeFURre1n",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LFPqoC_nVj8G",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "4bd450e6-60c3-476c-a189-71151ff25ea9"
      },
      "source": [
        "model.summary()"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "conv2d_1 (Conv2D)            (None, 64, 64, 32)        4736      \n",
            "_________________________________________________________________\n",
            "max_pooling2d_1 (MaxPooling2 (None, 32, 32, 32)        0         \n",
            "_________________________________________________________________\n",
            "conv2d_2 (Conv2D)            (None, 32, 32, 64)        51264     \n",
            "_________________________________________________________________\n",
            "max_pooling2d_2 (MaxPooling2 (None, 16, 16, 64)        0         \n",
            "_________________________________________________________________\n",
            "conv2d_3 (Conv2D)            (None, 16, 16, 64)        36928     \n",
            "_________________________________________________________________\n",
            "max_pooling2d_3 (MaxPooling2 (None, 8, 8, 64)          0         \n",
            "_________________________________________________________________\n",
            "conv2d_4 (Conv2D)            (None, 8, 8, 128)         73856     \n",
            "_________________________________________________________________\n",
            "max_pooling2d_4 (MaxPooling2 (None, 4, 4, 128)         0         \n",
            "_________________________________________________________________\n",
            "conv2d_5 (Conv2D)            (None, 4, 4, 128)         147584    \n",
            "_________________________________________________________________\n",
            "max_pooling2d_5 (MaxPooling2 (None, 2, 2, 128)         0         \n",
            "_________________________________________________________________\n",
            "conv2d_6 (Conv2D)            (None, 2, 2, 256)         295168    \n",
            "_________________________________________________________________\n",
            "max_pooling2d_6 (MaxPooling2 (None, 1, 1, 256)         0         \n",
            "_________________________________________________________________\n",
            "conv2d_7 (Conv2D)            (None, 1, 1, 256)         590080    \n",
            "_________________________________________________________________\n",
            "max_pooling2d_7 (MaxPooling2 (None, 1, 1, 256)         0         \n",
            "_________________________________________________________________\n",
            "dropout_1 (Dropout)          (None, 1, 1, 256)         0         \n",
            "_________________________________________________________________\n",
            "conv2d_8 (Conv2D)            (None, 1, 1, 512)         1180160   \n",
            "_________________________________________________________________\n",
            "max_pooling2d_8 (MaxPooling2 (None, 1, 1, 512)         0         \n",
            "_________________________________________________________________\n",
            "dropout_2 (Dropout)          (None, 1, 1, 512)         0         \n",
            "_________________________________________________________________\n",
            "flatten_1 (Flatten)          (None, 512)               0         \n",
            "_________________________________________________________________\n",
            "dense_1 (Dense)              (None, 256)               131328    \n",
            "_________________________________________________________________\n",
            "dense_2 (Dense)              (None, 64)                16448     \n",
            "_________________________________________________________________\n",
            "dense_3 (Dense)              (None, 32)                2080      \n",
            "_________________________________________________________________\n",
            "dense_4 (Dense)              (None, 16)                528       \n",
            "_________________________________________________________________\n",
            "dense_5 (Dense)              (None, 16)                272       \n",
            "_________________________________________________________________\n",
            "dense_6 (Dense)              (None, 8)                 136       \n",
            "_________________________________________________________________\n",
            "dropout_3 (Dropout)          (None, 8)                 0         \n",
            "_________________________________________________________________\n",
            "dense_7 (Dense)              (None, 8)                 72        \n",
            "_________________________________________________________________\n",
            "dropout_4 (Dropout)          (None, 8)                 0         \n",
            "_________________________________________________________________\n",
            "dense_8 (Dense)              (None, 4)                 36        \n",
            "=================================================================\n",
            "Total params: 2,530,676\n",
            "Trainable params: 2,530,676\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "74PccX8coNHb",
        "colab_type": "text"
      },
      "source": [
        "## 3번 model.*fitting*"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3D-yBz5VKRdO",
        "colab_type": "text"
      },
      "source": [
        "**model.fit_generator**\n",
        "\n",
        "- 첫번째 인자 : 훈련데이터셋을 제공할 제네레이터를 지정합니다.\n",
        "\n",
        "- steps_per_epoch : 한 epoch에 사용한 스텝 수를 지정합니다. \n",
        "\n",
        "- epochs : 전체 훈련 데이터셋에 대해 학습 반복 횟수를 지정합니다. \n",
        "\n",
        "- validation_data : 검증데이터셋을 제공할 제네레이터를 지정합니다. \n",
        "\n",
        "- validation_steps : 한 epoch 종료 시 마다 검증할 때 사용되는 검증 스텝 수를 지정합니다. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LUfA4nyKh51L",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "a10a9f48-5075-48a2-8beb-e16c0d7e82bb"
      },
      "source": [
        "\n",
        "history = model.fit(X_train, y_train, \n",
        "                    batch_size=32, epochs=100, \n",
        "                    validation_data=(X_test, y_test))\n",
        "                    "
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/math_ops.py:3066: to_int32 (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use tf.cast instead.\n",
            "Train on 8233 samples, validate on 3529 samples\n",
            "Epoch 1/100\n",
            "8233/8233 [==============================] - 288s 35ms/step - loss: 1.1760 - acc: 0.5732 - val_loss: 1.2339 - val_acc: 0.5769\n",
            "Epoch 2/100\n",
            "8233/8233 [==============================] - 287s 35ms/step - loss: 1.0745 - acc: 0.5810 - val_loss: 0.9679 - val_acc: 0.5965\n",
            "Epoch 3/100\n",
            "8233/8233 [==============================] - 286s 35ms/step - loss: 0.9709 - acc: 0.6078 - val_loss: 0.9593 - val_acc: 0.6180\n",
            "Epoch 4/100\n",
            "8233/8233 [==============================] - 287s 35ms/step - loss: 0.9554 - acc: 0.6073 - val_loss: 1.1042 - val_acc: 0.4769\n",
            "Epoch 5/100\n",
            "8233/8233 [==============================] - 285s 35ms/step - loss: 0.8871 - acc: 0.6382 - val_loss: 0.8514 - val_acc: 0.6741\n",
            "Epoch 6/100\n",
            "8233/8233 [==============================] - 287s 35ms/step - loss: 0.8403 - acc: 0.6774 - val_loss: 0.7632 - val_acc: 0.7110\n",
            "Epoch 7/100\n",
            "8233/8233 [==============================] - 286s 35ms/step - loss: 0.8070 - acc: 0.7018 - val_loss: 0.7369 - val_acc: 0.7169\n",
            "Epoch 8/100\n",
            "8233/8233 [==============================] - 287s 35ms/step - loss: 0.7423 - acc: 0.7191 - val_loss: 0.7885 - val_acc: 0.7039\n",
            "Epoch 9/100\n",
            "8233/8233 [==============================] - 285s 35ms/step - loss: 0.7347 - acc: 0.7220 - val_loss: 0.7199 - val_acc: 0.7308\n",
            "Epoch 10/100\n",
            "8233/8233 [==============================] - 289s 35ms/step - loss: 0.7020 - acc: 0.7314 - val_loss: 0.7257 - val_acc: 0.7254\n",
            "Epoch 11/100\n",
            "8233/8233 [==============================] - 288s 35ms/step - loss: 0.6682 - acc: 0.7403 - val_loss: 0.7011 - val_acc: 0.7331\n",
            "Epoch 12/100\n",
            "8233/8233 [==============================] - 285s 35ms/step - loss: 0.6494 - acc: 0.7476 - val_loss: 0.6625 - val_acc: 0.7481\n",
            "Epoch 13/100\n",
            "8233/8233 [==============================] - 287s 35ms/step - loss: 0.6220 - acc: 0.7495 - val_loss: 0.6688 - val_acc: 0.7410\n",
            "Epoch 14/100\n",
            "8233/8233 [==============================] - 286s 35ms/step - loss: 0.6104 - acc: 0.7517 - val_loss: 0.7156 - val_acc: 0.7268\n",
            "Epoch 15/100\n",
            "8233/8233 [==============================] - 288s 35ms/step - loss: 0.5659 - acc: 0.7698 - val_loss: 0.6508 - val_acc: 0.7504\n",
            "Epoch 16/100\n",
            "8233/8233 [==============================] - 290s 35ms/step - loss: 0.5450 - acc: 0.7744 - val_loss: 0.6359 - val_acc: 0.7611\n",
            "Epoch 17/100\n",
            "8233/8233 [==============================] - 290s 35ms/step - loss: 0.5433 - acc: 0.7738 - val_loss: 0.6978 - val_acc: 0.7668\n",
            "Epoch 18/100\n",
            "8233/8233 [==============================] - 290s 35ms/step - loss: 0.4999 - acc: 0.7947 - val_loss: 0.8135 - val_acc: 0.6948\n",
            "Epoch 19/100\n",
            "8233/8233 [==============================] - 290s 35ms/step - loss: 0.4898 - acc: 0.8000 - val_loss: 0.8413 - val_acc: 0.6265\n",
            "Epoch 20/100\n",
            "8233/8233 [==============================] - 289s 35ms/step - loss: 0.4533 - acc: 0.8109 - val_loss: 0.8614 - val_acc: 0.7489\n",
            "Epoch 21/100\n",
            "8233/8233 [==============================] - 291s 35ms/step - loss: 0.4171 - acc: 0.8261 - val_loss: 0.8925 - val_acc: 0.7589\n",
            "Epoch 22/100\n",
            "8233/8233 [==============================] - 290s 35ms/step - loss: 0.3994 - acc: 0.8319 - val_loss: 0.8759 - val_acc: 0.7682\n",
            "Epoch 23/100\n",
            "8233/8233 [==============================] - 290s 35ms/step - loss: 0.4500 - acc: 0.8191 - val_loss: 0.9091 - val_acc: 0.7240\n",
            "Epoch 24/100\n",
            "8233/8233 [==============================] - 288s 35ms/step - loss: 0.4087 - acc: 0.8358 - val_loss: 0.8665 - val_acc: 0.7376\n",
            "Epoch 25/100\n",
            "8233/8233 [==============================] - 287s 35ms/step - loss: 0.3666 - acc: 0.8411 - val_loss: 0.9058 - val_acc: 0.7470\n",
            "Epoch 26/100\n",
            "8233/8233 [==============================] - 286s 35ms/step - loss: 0.3923 - acc: 0.8335 - val_loss: 0.9366 - val_acc: 0.7365\n",
            "Epoch 27/100\n",
            "8233/8233 [==============================] - 287s 35ms/step - loss: 0.3849 - acc: 0.8354 - val_loss: 1.1761 - val_acc: 0.7209\n",
            "Epoch 28/100\n",
            "8233/8233 [==============================] - 290s 35ms/step - loss: 0.3019 - acc: 0.8731 - val_loss: 0.8996 - val_acc: 0.7560\n",
            "Epoch 29/100\n",
            "8233/8233 [==============================] - 287s 35ms/step - loss: 0.2962 - acc: 0.8833 - val_loss: 0.9419 - val_acc: 0.7855\n",
            "Epoch 30/100\n",
            "8233/8233 [==============================] - 287s 35ms/step - loss: 0.2675 - acc: 0.8987 - val_loss: 1.1068 - val_acc: 0.7651\n",
            "Epoch 31/100\n",
            "1952/8233 [======>.......................] - ETA: 3:11 - loss: 0.2697 - acc: 0.8945"
          ],
          "name": "stdout"
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-7-0ba17067cff3>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      2\u001b[0m history = model.fit(X_train, y_train, \n\u001b[1;32m      3\u001b[0m                     \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m32\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m100\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m                     validation_data=(X_test, y_test)) \n\u001b[0m\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/keras/engine/training.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, **kwargs)\u001b[0m\n\u001b[1;32m   1037\u001b[0m                                         \u001b[0minitial_epoch\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minitial_epoch\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1038\u001b[0m                                         \u001b[0msteps_per_epoch\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msteps_per_epoch\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1039\u001b[0;31m                                         validation_steps=validation_steps)\n\u001b[0m\u001b[1;32m   1040\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1041\u001b[0m     def evaluate(self, x=None, y=None,\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/keras/engine/training_arrays.py\u001b[0m in \u001b[0;36mfit_loop\u001b[0;34m(model, f, ins, out_labels, batch_size, epochs, verbose, callbacks, val_f, val_ins, shuffle, callback_metrics, initial_epoch, steps_per_epoch, validation_steps)\u001b[0m\n\u001b[1;32m    197\u001b[0m                     \u001b[0mins_batch\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mins_batch\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtoarray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    198\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 199\u001b[0;31m                 \u001b[0mouts\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mins_batch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    200\u001b[0m                 \u001b[0mouts\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mto_list\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mouts\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    201\u001b[0m                 \u001b[0;32mfor\u001b[0m \u001b[0ml\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mo\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mzip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mout_labels\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mouts\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m   2713\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_legacy_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2714\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2715\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2716\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2717\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mpy_any\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mis_tensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py\u001b[0m in \u001b[0;36m_call\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m   2673\u001b[0m             \u001b[0mfetched\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_callable_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0marray_vals\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun_metadata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2674\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2675\u001b[0;31m             \u001b[0mfetched\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_callable_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0marray_vals\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2676\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mfetched\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2677\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1437\u001b[0m           ret = tf_session.TF_SessionRunCallable(\n\u001b[1;32m   1438\u001b[0m               \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_session\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_handle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstatus\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1439\u001b[0;31m               run_metadata_ptr)\n\u001b[0m\u001b[1;32m   1440\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1441\u001b[0m           \u001b[0mproto_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BITyR1WVjTJ8",
        "colab_type": "text"
      },
      "source": [
        "### 예측 만들기"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jt3gP9Wcjaph",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 58
        },
        "outputId": "2cee0074-8195-4e9a-e8b9-b6f46cc8e2e8"
      },
      "source": [
        "print(\"정확도 : %.4f\" % (model.evaluate(X_test, y_test)[1]))"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "3529/3529 [==============================] - 36s 10ms/step\n",
            "정확도 : 0.7710\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GrVWxw20K-tG",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fOcIWHLgWMIS",
        "colab_type": "text"
      },
      "source": [
        ""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dQyZ1iPQOyu7",
        "colab_type": "text"
      },
      "source": [
        "## 4번 모델을 이용해 새로운 이미지를 분류해보자"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eiATXxnBj3oR",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from PIL import Image\n",
        "import os, glob, numpy as np\n",
        "from keras.models import load_model\n",
        "\n",
        "caltech_dir = \"/content/drive/My Drive/CTRC/train/multi_img_data/imgs_others_test\"\n",
        "image_w = 64\n",
        "image_h = 64\n",
        "\n",
        "pixels = image_h * image_w * 3\n",
        "\n",
        "\n",
        "#테스트할 이미지를 변환할 소스\n",
        "X = []\n",
        "filenames = []\n",
        "files = glob.glob(caltech_dir+\"/*.*\")\n",
        "for i, f in enumerate(files):\n",
        "    img = Image.open(f)\n",
        "    img = img.convert(\"RGB\")\n",
        "    img = img.resize((image_w, image_h))\n",
        "    data = np.asarray(img)\n",
        "    filenames.append(f)\n",
        "    X.append(data)\n",
        "\n",
        "X = np.array(X)\n",
        "X = X.reshape(X.shape[0], 64,64,3)\n",
        "#model = load_model( './model/multi_img_classification.model')\n",
        "\n",
        "\n",
        "#해당 이미지 predict\n",
        "prediction = model.predict(X)\n",
        "np.set_printoptions(formatter={'float': lambda x: \"{0:0.3f}\".format(x)})\n",
        "cnt = 0"
      ],
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "n06sVosJj3lM",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "atilE7sX0yGz",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 226
        },
        "outputId": "a3da2bec-fb6d-4567-8979-b8c0452bf5d5"
      },
      "source": [
        "#저장된 모델구조와 모델 가중치를 불러옵니다.\n",
        "\n",
        "from keras.models import load_model\n",
        "\n",
        "#model=load_model('project.h5')\n",
        "\n",
        "\n",
        "\n",
        "trainGen = imageGenerator.flow_from_directory('/content/drive/My Drive/CTRC/train',\n",
        "                                                  target_size=(64,64),\n",
        "                                                  batch_size=3,\n",
        "                                                   class_mode='categorical')\n",
        "\n",
        "\n",
        "\n",
        "print(\"--Predict--\")\n",
        "\n",
        "output=model.predict_generator(trainGen,steps=220)\n",
        "\n",
        "np.set_printoptions(formatter={'float':lambda x:\"{0:0.3f}\".format(x)})\n",
        "\n",
        "print(trainGen.class_indices)\n",
        "\n",
        "print(output)\n"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Found 11762 images belonging to 4 classes.\n",
            "--Predict--\n",
            "{'1. Cancer': 0, '2. Precancer': 1, '3. Extra': 2, '4. Normal': 3}\n",
            "[[0.214 0.513 0.254 0.019]\n",
            " [0.000 0.000 0.057 0.943]\n",
            " [0.589 0.389 0.022 0.000]\n",
            " ...\n",
            " [0.884 0.116 0.000 0.000]\n",
            " [0.000 0.000 0.523 0.477]\n",
            " [0.006 0.144 0.706 0.144]]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZpOWX84PoKT4",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from tensorflow.keras.preprocessing.image import array_to_img\n",
        "import numpy as np\n",
        "\n",
        "cls_index = ['1. Cancer', '2. Precancer','3. Extra','4. Normal']\n",
        "\n",
        "imgs = testGen.next()\n",
        "arr = imgs[0][0]\n",
        "\n",
        "\n",
        "img = array_to_img(arr).resize((128, 128))"
      ],
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PWR7EmL-oKQ7",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "D8JCzkgVK3Iu",
        "colab_type": "text"
      },
      "source": [
        "### 성능 그래프"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uJH-rKjKWCuy",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 282
        },
        "outputId": "865e0d16-c7d3-4aca-a744-24703938e3d6"
      },
      "source": [
        "y_vloss = history.history['val_loss']\n",
        "y_loss = history.history['loss']\n",
        "\n",
        "x_len = np.arange(len(y_loss))\n",
        "\n",
        "plt.plot(x_len, y_vloss, marker='.', c='red', label='val_set_loss')\n",
        "plt.plot(x_len, y_loss, marker='.', c='blue', label='train_set_oss')\n",
        "plt.legend()\n",
        "plt.xlabel('epochs')\n",
        "plt.ylabel('loss')\n",
        "plt.grid()\n",
        "plt.show()"
      ],
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-19-b60132607313>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0my_vloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhistory\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhistory\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'val_loss'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0my_loss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhistory\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhistory\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'loss'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mx_len\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_loss\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'history' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "m1hkvrJRoeId",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 303
        },
        "outputId": "acf8db71-6161-4c6d-ebbb-17555e7971aa"
      },
      "source": [
        "plt.clf() # 그래프를 초기화합니다.\n",
        "acc = history.history['acc']\n",
        "val_acc = history.history['val_acc']\n",
        "\n",
        "plt.plot(x_len, acc, 'bo', label='Training acc')\n",
        "plt.plot(x_len, val_acc, 'b', label='Validation acc')\n",
        "plt.title('Training and validation accuracy')\n",
        "plt.xlabel('Epochs')\n",
        "plt.ylabel('Accuracy')\n",
        "plt.legend()\n",
        "\n",
        "plt.show()"
      ],
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-20-a980b134aeb0>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m# 그래프를 초기화합니다.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0macc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhistory\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhistory\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'acc'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0mval_acc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhistory\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhistory\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'val_acc'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mplot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx_len\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0macc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'bo'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabel\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'Training acc'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'history' is not defined"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 432x288 with 0 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ic4vQQoUooLt",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 453
        },
        "outputId": "9f242d21-2230-4672-d828-be48979d4e8a"
      },
      "source": [
        "import numpy as np\n",
        "import sklearn.metrics as metrics\n",
        "\n",
        "\n",
        "\n",
        "y = trainGen \n",
        "\n",
        "p = output\n",
        "\n",
        "\n",
        "\n",
        "accuracy = np.mean(np.equal(y,p))\n",
        "\n",
        "right = np.sum(y * p == 1)\n",
        "\n",
        "precision = right / np.sum(p)\n",
        "\n",
        "recall = right / np.sum(y)\n",
        "\n",
        "f1 = 2 * precision*recall/(precision+recall)\n",
        "\n",
        "\n",
        "\n",
        "print('accuracy',accuracy)\n",
        "\n",
        "print('precision', precision)\n",
        "\n",
        "print('recall', recall)\n",
        "\n",
        "print('f1', f1)\n",
        "\n",
        "\n",
        "\n",
        "# sklearn 을 이용하면 전부 계산해준다.\n",
        "\n",
        "print('accuracy', metrics.accuracy_score(y,p) )\n",
        "\n",
        "print('precision', metrics.precision_score(y,p) )\n",
        "\n",
        "print('recall', metrics.recall_score(y,p) )\n",
        "\n",
        "print('f1', metrics.f1_score(y,p) )\n"
      ],
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-21-772769e8a738>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 12\u001b[0;31m \u001b[0maccuracy\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmean\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mequal\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mp\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     13\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m \u001b[0mright\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mp\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/keras_preprocessing/image/iterator.py\u001b[0m in \u001b[0;36m__next__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    102\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    103\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__next__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 104\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    105\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    106\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mnext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/keras_preprocessing/image/iterator.py\u001b[0m in \u001b[0;36mnext\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    114\u001b[0m         \u001b[0;31m# The transformation of images is not under thread lock\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    115\u001b[0m         \u001b[0;31m# so it can be done in parallel\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 116\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_batches_of_transformed_samples\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mindex_array\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    117\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    118\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_get_batches_of_transformed_samples\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mindex_array\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/keras_preprocessing/image/iterator.py\u001b[0m in \u001b[0;36m_get_batches_of_transformed_samples\u001b[0;34m(self, index_array)\u001b[0m\n\u001b[1;32m    228\u001b[0m                            \u001b[0mcolor_mode\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcolor_mode\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    229\u001b[0m                            \u001b[0mtarget_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtarget_size\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 230\u001b[0;31m                            interpolation=self.interpolation)\n\u001b[0m\u001b[1;32m    231\u001b[0m             \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mimg_to_array\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimg\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata_format\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata_format\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    232\u001b[0m             \u001b[0;31m# Pillow images should be closed after `load_img`,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/keras_preprocessing/image/utils.py\u001b[0m in \u001b[0;36mload_img\u001b[0;34m(path, grayscale, color_mode, target_size, interpolation)\u001b[0m\n\u001b[1;32m    136\u001b[0m                             \", \".join(_PIL_INTERPOLATION_METHODS.keys())))\n\u001b[1;32m    137\u001b[0m                 \u001b[0mresample\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_PIL_INTERPOLATION_METHODS\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0minterpolation\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 138\u001b[0;31m                 \u001b[0mimg\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mimg\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mresize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mwidth_height_tuple\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresample\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    139\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mimg\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    140\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/PIL/Image.py\u001b[0m in \u001b[0;36mresize\u001b[0;34m(self, size, resample, box, reducing_gap)\u001b[0m\n\u001b[1;32m   1856\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mim\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconvert\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmode\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1857\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1858\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1859\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1860\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mreducing_gap\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mresample\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0mNEAREST\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/PIL/ImageFile.py\u001b[0m in \u001b[0;36mload\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    249\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    250\u001b[0m                             \u001b[0mb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mb\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0ms\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 251\u001b[0;31m                             \u001b[0mn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0merr_code\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdecoder\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdecode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mb\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    252\u001b[0m                             \u001b[0;32mif\u001b[0m \u001b[0mn\u001b[0m \u001b[0;34m<\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    253\u001b[0m                                 \u001b[0;32mbreak\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "j2IJ8ebWrsIz",
        "colab_type": "text"
      },
      "source": [
        "## 5번 혼동 행렬"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZzXtI-lwnshO",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 37
        },
        "outputId": "008867aa-5d2e-4892-8228-9f42922e9868"
      },
      "source": [
        "testGen = imageGenerator.flow_from_directory('/content/drive/My Drive/CTRC/test',\n",
        "                                                  target_size=(64,64),\n",
        "                                                  batch_size=3,\n",
        "                                                   class_mode='categorical')"
      ],
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Found 1961 images belonging to 4 classes.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sjuTFWDHo6Q9",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 198
        },
        "outputId": "2f5e5339-ef10-4652-b700-4af1d6304c05"
      },
      "source": [
        "output2=model.predict_generator(testGen,steps=3)\n"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-1-569a0fb24792>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0moutput2\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict_generator\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtestGen\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0msteps\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m: name 'model' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "s1eX23zDBbgz",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 432
        },
        "outputId": "8e212091-738c-46c5-b259-e3e7d9314cbc"
      },
      "source": [
        "\n",
        "'''from sklearn.metrics import confusion_matrix\n",
        "\n",
        "y_test = [2, 0, 2, 2, 0, 1] #실제값\n",
        "y_pred = [0, 0, 2, 2, 0, 2] #예측값\n",
        "\n",
        "conf_matrix = confusion_matrix(y_test, y_pred) #혼동 행렬\n",
        "print(conf_matrix)'''\n",
        "\n",
        "\n",
        "from sklearn.metrics import confusion_matrix\n",
        "\n",
        "y_test = testGen #내 코드에서 수정\n",
        "y_pred = output2\n",
        "\n",
        "\n",
        "conf_matrix = confusion_matrix(y_test, y_pred) \n",
        "print(conf_matrix)"
      ],
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "error",
          "ename": "ValueError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-25-dc3879638996>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     15\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 17\u001b[0;31m \u001b[0mconf_matrix\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mconfusion_matrix\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_test\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moutput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     18\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mconf_matrix\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/sklearn/metrics/_classification.py\u001b[0m in \u001b[0;36mconfusion_matrix\u001b[0;34m(y_true, y_pred, labels, sample_weight, normalize)\u001b[0m\n\u001b[1;32m    266\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    267\u001b[0m     \"\"\"\n\u001b[0;32m--> 268\u001b[0;31m     \u001b[0my_type\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_true\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_pred\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_check_targets\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_true\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_pred\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    269\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0my_type\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32min\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;34m\"binary\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"multiclass\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    270\u001b[0m         \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"%s is not supported\"\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0my_type\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/sklearn/metrics/_classification.py\u001b[0m in \u001b[0;36m_check_targets\u001b[0;34m(y_true, y_pred)\u001b[0m\n\u001b[1;32m     78\u001b[0m     \u001b[0my_pred\u001b[0m \u001b[0;34m:\u001b[0m \u001b[0marray\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mindicator\u001b[0m \u001b[0mmatrix\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     79\u001b[0m     \"\"\"\n\u001b[0;32m---> 80\u001b[0;31m     \u001b[0mcheck_consistent_length\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_true\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_pred\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     81\u001b[0m     \u001b[0mtype_true\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtype_of_target\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_true\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     82\u001b[0m     \u001b[0mtype_pred\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtype_of_target\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_pred\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/sklearn/utils/validation.py\u001b[0m in \u001b[0;36mcheck_consistent_length\u001b[0;34m(*arrays)\u001b[0m\n\u001b[1;32m    210\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0muniques\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    211\u001b[0m         raise ValueError(\"Found input variables with inconsistent numbers of\"\n\u001b[0;32m--> 212\u001b[0;31m                          \" samples: %r\" % [int(l) for l in lengths])\n\u001b[0m\u001b[1;32m    213\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    214\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mValueError\u001b[0m: Found input variables with inconsistent numbers of samples: [654, 660]"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fTO0zXZ-rxPW",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 261
        },
        "outputId": "689722ea-1536-4add-dab1-b6a9b1106413"
      },
      "source": [
        "\n",
        "title = 'test'\n",
        "cmap=plt.cm.Greens\n",
        "plt.figure(figsize=(6, 6))\n",
        "plt.imshow(conf_matrix, interpolation='nearest', cmap=cmap)  # , cmap=plt.cm.Greens\n",
        "plt.title(title, size=12)\n",
        "plt.colorbar(fraction=0.05, pad=0.05)\n",
        "tick_marks = np.arange(3, 3)\n",
        "plt.xticks(np.arange(4), (\"1. Cancer\",\"2. Precancer\",\"3. Extra\",\"4. Normal\"))\n",
        "plt.yticks(np.arange(4), (\"1. Cancer\",\"2. Precancer\",\"3. Extra\",\"4. Normal\"))\n",
        "\n",
        "fmt = 'd' \n",
        "thresh = 1\n",
        "for i in range(conf_matrix.shape[0]):\n",
        "    for j in range(conf_matrix.shape[1]):\n",
        "        plt.text(j, i, format(conf_matrix[i, j], fmt),\n",
        "                 ha=\"center\", va=\"center\", \n",
        "                 color=\"white\" if conf_matrix[i, j] > thresh else \"black\")  #horizontalalignment"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-32-c9d9374dac4f>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mcmap\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcm\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mGreens\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfigure\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfigsize\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m6\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m6\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mimshow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mconf_matrix\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minterpolation\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'nearest'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcmap\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcmap\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# , cmap=plt.cm.Greens\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      6\u001b[0m \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtitle\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtitle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msize\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m12\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcolorbar\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfraction\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0.05\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpad\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0.05\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'conf_matrix' is not defined"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 432x432 with 0 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ph43TapLdCBm",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}
