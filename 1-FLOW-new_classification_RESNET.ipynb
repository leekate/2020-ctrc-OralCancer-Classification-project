{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "new_classification_RESNET.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/leekate/2020ctrc/blob/master/new_classification_RESNET.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2zuwHf6FUgXL",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "29d77fab-514d-4f78-f374-27a3afa4f1c6"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Mounted at /content/drive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9V8MH55jEaNW",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "083377e3-758e-4de5-ee24-5213a6fb7c10"
      },
      "source": [
        "pip install dataset"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting dataset\n",
            "  Downloading https://files.pythonhosted.org/packages/98/37/7d5a3d47b663173af4066ef761462de2fbd22a893e0aa659ab07540e90b2/dataset-1.4.1-py2.py3-none-any.whl\n",
            "Collecting banal>=1.0.1\n",
            "  Downloading https://files.pythonhosted.org/packages/3a/a0/6e64c775272209016bbd06de0c1f02388780a91e174aa79c4cdbab4968b5/banal-1.0.1-py2.py3-none-any.whl\n",
            "Collecting alembic>=0.6.2\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/12/aa/c261dfd7f4ba6ce4701846a2689a46e2a172e012171de4378fc2926e3bf0/alembic-1.4.3-py2.py3-none-any.whl (159kB)\n",
            "\u001b[K     |████████████████████████████████| 163kB 11.5MB/s \n",
            "\u001b[?25hRequirement already satisfied: sqlalchemy>=1.3.2 in /usr/local/lib/python3.6/dist-packages (from dataset) (1.3.20)\n",
            "Collecting python-editor>=0.3\n",
            "  Downloading https://files.pythonhosted.org/packages/c6/d3/201fc3abe391bbae6606e6f1d598c15d367033332bd54352b12f35513717/python_editor-1.0.4-py3-none-any.whl\n",
            "Requirement already satisfied: python-dateutil in /usr/local/lib/python3.6/dist-packages (from alembic>=0.6.2->dataset) (2.8.1)\n",
            "Collecting Mako\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/a6/37/0e706200d22172eb8fa17d68a7ae22dec7631a0a92266634fb518a88a5b2/Mako-1.1.3-py2.py3-none-any.whl (75kB)\n",
            "\u001b[K     |████████████████████████████████| 81kB 10.2MB/s \n",
            "\u001b[?25hRequirement already satisfied: six>=1.5 in /usr/local/lib/python3.6/dist-packages (from python-dateutil->alembic>=0.6.2->dataset) (1.15.0)\n",
            "Requirement already satisfied: MarkupSafe>=0.9.2 in /usr/local/lib/python3.6/dist-packages (from Mako->alembic>=0.6.2->dataset) (1.1.1)\n",
            "Installing collected packages: banal, python-editor, Mako, alembic, dataset\n",
            "Successfully installed Mako-1.1.3 alembic-1.4.3 banal-1.0.1 dataset-1.4.1 python-editor-1.0.4\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "C7jOTcBCaH-f"
      },
      "source": [
        "# Data Load, Read"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pAasPFcSXO0E"
      },
      "source": [
        "\n",
        "import os\n",
        "import glob\n",
        "import numpy as np\n",
        "import cv2\n",
        "from sklearn.utils import shuffle\n",
        "\n",
        "def load_train(train_path, image_size, classes):\n",
        "    images = []\n",
        "    labels = []\n",
        "    ids = []\n",
        "    cls = []\n",
        "\n",
        "    print('Reading training images')\n",
        "    for fld in classes:\n",
        "        index = classes.index(fld)\n",
        "        print('Loading {} files (Index: {})'.format(fld, index))\n",
        "        path = os.path.join(train_path, fld, '*g')\n",
        "        files = glob.glob(path)\n",
        "        for fl in files:\n",
        "            image = cv2.imread(fl)\n",
        "            image = cv2.resize(image, (image_size, image_size), cv2.INTER_LINEAR)\n",
        "            images.append(image)\n",
        "            label = np.zeros(len(classes))\n",
        "            label[index] = 1.0\n",
        "            labels.append(label)\n",
        "            flbase = os.path.basename(fl)\n",
        "            ids.append(flbase)\n",
        "            cls.append(fld)\n",
        "    images = np.array(images)\n",
        "    labels = np.array(labels)\n",
        "    ids = np.array(ids)\n",
        "    cls = np.array(cls)\n",
        "  \n",
        "\n",
        "    return images, labels, ids, cls\n",
        "\n",
        "\n",
        "def load_test(test_path, image_size):\n",
        "    path = os.path.join(test_path, '*g')\n",
        "    files = sorted(glob.glob(path))\n",
        "\n",
        "    X_test = []\n",
        "    X_test_id = []\n",
        "    print(\"Reading test images\")\n",
        "    for fl in files:\n",
        "        \n",
        "        img = cv2.imread(fl)\n",
        "        img = cv2.resize(img, (image_size, image_size), cv2.INTER_LINEAR)\n",
        "        X_test.append(img)\n",
        "        X_test_id.append(flbase)\n",
        "        flbase = os.path.basename(fl)\n",
        "\n",
        "    X_test = np.array(X_test, dtype=np.uint8)\n",
        "    X_test = X_test.astype('float32')\n",
        "    X_test = X_test / 255\n",
        "    X_test_id = np.array(X_test_id)\n",
        "\n",
        "    return X_test, X_test_id\n",
        "\n",
        "\n",
        "class DataSet(object):\n",
        "\n",
        "    def __init__(self, images, labels, ids, cls):\n",
        "        \"\"\"Construct a DataSet. one_hot arg is used only if fake_data is true.\"\"\"\n",
        "\n",
        "        self._num_examples = images.shape[0]\n",
        "\n",
        "        # Convert shape from [num examples, rows, columns, depth]\n",
        "        # to [num examples, rows*columns] (assuming depth == 1)\n",
        "        # Convert from [0, 255] -> [0.0, 1.0].\n",
        "\n",
        "        images = images.astype(np.float32)\n",
        "        images = np.multiply(images, 1.0 / 255.0)\n",
        "\n",
        "        self._images = images\n",
        "        self._labels = labels\n",
        "        self._ids = ids\n",
        "        self._cls = cls\n",
        "        self._epochs_completed = 0\n",
        "        self._index_in_epoch = 0\n",
        "\n",
        "    @property\n",
        "    def images(self):\n",
        "        return self._images\n",
        "\n",
        "    @property\n",
        "    def labels(self):\n",
        "        return self._labels\n",
        "\n",
        "    @property\n",
        "    def ids(self):\n",
        "        return self._ids\n",
        "\n",
        "    @property\n",
        "    def cls(self):\n",
        "        return self._cls\n",
        "\n",
        "    @property\n",
        "    def num_examples(self):\n",
        "        return self._num_examples\n",
        "\n",
        "    @property\n",
        "    def epochs_completed(self):\n",
        "        return self._epochs_completed\n",
        "\n",
        "    def next_batch(self, batch_size):\n",
        "        \"\"\"Return the next `batch_size` examples from this data set.\"\"\"\n",
        "        start = self._index_in_epoch\n",
        "        self._index_in_epoch += batch_size\n",
        "\n",
        "        if self._index_in_epoch > self._num_examples:\n",
        "            # Finished epoch\n",
        "            self._epochs_completed += 1\n",
        "\n",
        "            # # Shuffle the data (maybe)\n",
        "            # perm = np.arange(self._num_examples)\n",
        "            # np.random.shuffle(perm)\n",
        "            # self._images = self._images[perm]\n",
        "            # self._labels = self._labels[perm]\n",
        "            # Start next epoch\n",
        "\n",
        "            start = 0\n",
        "            self._index_in_epoch = batch_size\n",
        "            assert batch_size <= self._num_examples\n",
        "        end = self._index_in_epoch\n",
        "\n",
        "        return self._images[start:end], self._labels[start:end], self._ids[start:end], self._cls[start:end]\n",
        "\n",
        "\n",
        "def read_train_sets(train_path, image_size, classes, validation_size=0):\n",
        "    class DataSets(object):\n",
        "        pass\n",
        "\n",
        "    data_sets = DataSets()\n",
        "\n",
        "    images, labels, ids, cls = load_train(train_path, image_size, classes)\n",
        "    images, labels, ids, cls = shuffle(images, labels, ids, cls)  # shuffle the data\n",
        "\n",
        "    if isinstance(validation_size, float):\n",
        "        validation_size = int(validation_size * images.shape[0])\n",
        "\n",
        "    train_images = images\n",
        "    train_labels = labels\n",
        "    train_ids = ids\n",
        "    train_cls = cls\n",
        "\n",
        "    data_sets.train = DataSet(train_images, train_labels, train_ids, train_cls)\n",
        "\n",
        "    return data_sets"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xR6T7ZmRXP9j"
      },
      "source": [
        "def load_validation(validation_path, image_size, classes):\n",
        "    images2 = []\n",
        "    labels2 = []\n",
        "    ids2 = []\n",
        "    cls2 = []\n",
        "\n",
        "    print('Reading validation images')\n",
        "    for fld in classes:\n",
        "        index = classes.index(fld)\n",
        "        print('Loading {} files (Index: {})'.format(fld, index))\n",
        "        path = os.path.join(validation_path, fld, '*g')\n",
        "        files = glob.glob(path)\n",
        "        for fl in files:\n",
        "            image = cv2.imread(fl)\n",
        "            image = cv2.resize(image, (image_size, image_size), cv2.INTER_LINEAR)\n",
        "            images2.append(image)\n",
        "            label = np.zeros(len(classes))\n",
        "            label[index] = 1.0\n",
        "            labels2.append(label)\n",
        "            flbase = os.path.basename(fl)\n",
        "            ids2.append(flbase)\n",
        "            cls2.append(fld)\n",
        "    images2 = np.array(images2)\n",
        "    labels2 = np.array(labels2)\n",
        "    ids2 = np.array(ids2)\n",
        "    cls2 = np.array(cls2)\n",
        "  \n",
        "\n",
        "    return images2, labels2, ids2, cls2\n",
        "\n",
        "\n",
        "def read_validation_sets(validation_path, image_size, classes, validation_size=0):\n",
        "    class DataSets(object):\n",
        "        pass\n",
        "\n",
        "    data_sets = DataSets()\n",
        "\n",
        "    images2, labels2, ids2, cls2 = load_validation(validation_path, image_size, classes)\n",
        "    images2, labels2, ids2, cls2 = shuffle(images2, labels2, ids2, cls2)  # shuffle the data\n",
        "\n",
        "    if isinstance(validation_size, float):\n",
        "        validation_size = int(validation_size * images2.shape[0])\n",
        "\n",
        "    validation_images = images2\n",
        "    validation_labels = labels2\n",
        "    validation_ids = ids2\n",
        "    validation_cls = cls2\n",
        "\n",
        "    data_sets.valid = DataSet(validation_images, validation_labels, validation_ids, validation_cls)\n",
        "\n",
        "    return data_sets"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gwkxE2C4XSup"
      },
      "source": [
        "def load_test(test_path, image_size, classes):\n",
        "    images3 = []\n",
        "    labels3 = []\n",
        "    ids3 = []\n",
        "    cls3 = []\n",
        "\n",
        "    print('Reading validation images')\n",
        "    for fld in classes:\n",
        "        index = classes.index(fld)\n",
        "        print('Loading {} files (Index: {})'.format(fld, index))\n",
        "        path = os.path.join(test_path, fld, '*g')\n",
        "        files = glob.glob(path)\n",
        "        for fl in files:\n",
        "            image = cv2.imread(fl)\n",
        "            image = cv2.resize(image, (image_size, image_size), cv2.INTER_LINEAR)\n",
        "            images3.append(image)\n",
        "            label = np.zeros(len(classes))\n",
        "            label[index] = 1.0\n",
        "            labels3.append(label)\n",
        "            flbase = os.path.basename(fl)\n",
        "            ids3.append(flbase)\n",
        "            cls3.append(fld)\n",
        "    images3 = np.array(images3)\n",
        "    labels3 = np.array(labels3)\n",
        "    ids3 = np.array(ids3)\n",
        "    cls3 = np.array(cls3)\n",
        "  \n",
        "\n",
        "    return images3, labels3, ids3, cls3\n",
        "\n",
        "\n",
        "def read_test_sets(test_path, image_size, classes, validation_size=0):\n",
        "    class DataSets(object):\n",
        "        pass\n",
        "\n",
        "    data_sets = DataSets()\n",
        "\n",
        "    images3, labels3, ids3, cls3 = load_test(test_path, image_size, classes)\n",
        "    images3, labels3, ids3, cls3 = shuffle(images3, labels3, ids3, cls3)  # shuffle the data\n",
        "\n",
        "    if isinstance(validation_size, float):\n",
        "        validation_size = int(validation_size * images3.shape[0])\n",
        "\n",
        "    test_images = images3\n",
        "    test_labels = labels3\n",
        "    test_ids = ids3\n",
        "    test_cls = cls3\n",
        "\n",
        "    data_sets.test = DataSet(test_images, test_labels, test_ids, test_cls)\n",
        "\n",
        "    return data_sets"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XFEYiZ8baCnV"
      },
      "source": [
        ""
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ea1AV3d6XVIj",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "37561263-c912-47bb-8cd3-fdf775790c94"
      },
      "source": [
        "import time\n",
        "import math\n",
        "import random\n",
        "\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import tensorflow.compat.v1 as tf\n",
        "import cv2\n",
        "import dataset\n",
        "import os\n",
        "import keras\n",
        "\n",
        "from sklearn.metrics import confusion_matrix\n",
        "from datetime import timedelta\n",
        "import seaborn as sn\n",
        "\n",
        "%matplotlib inline\n",
        "\n",
        "tf.disable_v2_behavior()\n",
        "\n",
        "#######################################################################\n",
        "\n",
        "filter_size1 = 7\n",
        "num_filters1 = 64\n",
        "\n",
        "filter_size2 = 1\n",
        "num_filters2 = 64\n",
        "\n",
        "filter_size3 = 3\n",
        "num_filters3 = 64\n",
        "\n",
        "filter_size4 = 1\n",
        "num_filters4 = 256\n",
        "\n",
        "filter_size5 = 1\n",
        "num_filters5 = 128\n",
        "\n",
        "filter_size6 = 3\n",
        "num_filters6 = 128\n",
        "\n",
        "filter_size7 = 1\n",
        "num_filters7 = 512\n",
        "\n",
        "filter_size8 = 1\n",
        "num_filters8 = 256\n",
        "\n",
        "filter_size9 = 3\n",
        "num_filters9 = 256\n",
        "\n",
        "filter_size10 = 1\n",
        "num_filters10 = 1024\n",
        "\n",
        "filter_size11 = 1\n",
        "num_filters11 = 512\n",
        "\n",
        "filter_size12 = 3\n",
        "num_filters12 = 512\n",
        "\n",
        "filter_size13 = 1\n",
        "num_filters13 = 2048\n",
        "\n",
        "#######################################################################\n",
        "\n",
        "# Fully-connected layer.\n",
        "# Number of neurons in fully-connected layer.\n",
        "# convolution layer 전체에 있는 뉴런 수 = 필터 역할\n",
        "# fc_size = 256             \n",
        "fc_size = 1000\n",
        "\n",
        "\n",
        "# Number of color channels for the images: 1 channel for gray-scale.\n",
        "num_channels = 3\n",
        "\n",
        "# image dimensions (only squares for now)\n",
        "img_size = 224\n",
        "\n",
        "\n",
        "#이전 층으로부터 받은 데이터에 대해 \n",
        "#뉴런의 수만큼 해당되는 영역만큼 합성곱을 진행, 반복\n",
        "\n",
        "\n",
        "# Size of image when flattened to a single dimension\n",
        "img_size_flat = img_size * img_size * num_channels\n",
        "\n",
        "# Tuple with height and width of images used to reshape arrays.\n",
        "img_shape = (img_size, img_size)\n",
        "\n",
        "# class info\n",
        "classes = ['1.Cancer', '2.Precancer', '3.Inflammatory', '4.Normal']\n",
        "num_classes = len(classes)\n",
        "\n",
        "# batch size\n",
        "batch_size = 32\n",
        "\n",
        "# # validation split\n",
        "# validation_size = .2\n",
        "\n",
        "# how long to wait after validation loss stops improving before terminating training\n",
        "early_stopping = None  # use None if you don't want to implement early stoping"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow/python/compat/v2_compat.py:96: disable_resource_variables (from tensorflow.python.ops.variable_scope) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "non-resource variables are not supported in the long term\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5UHRTj6eXcuY",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "05300472-0251-4f87-bbae-bad0a384e8b1"
      },
      "source": [
        "\n",
        "#######################################################################\n",
        "# 공유드라이브 주소로 수정\n",
        "train_path = '/content/drive/Shareddrives/CTRC/train2'\n",
        "validation_path = '/content/drive/Shareddrives/CTRC/validation2'\n",
        "test_path = '/content/drive/Shareddrives/CTRC/test2'\n",
        "checkpoint_dir = '/content/drive/My Drive/CTRC/RESNET/model'\n",
        "\n",
        "\n",
        "#######################################################################\n",
        "\n",
        "# data = read_train_sets(train_path, img_size, classes, validation_size=validation_size)\n",
        "# data2 = read_validation_sets(validation_path, img_size, classes, validation_size=validation_size)\n",
        "# data3 = read_test_sets(test_path, img_size, classes, validation_size=validation_size)\n",
        "\n",
        "data = read_train_sets(train_path, img_size, classes, validation_size=0)\n",
        "data2 = read_validation_sets(validation_path, img_size, classes, validation_size=0)\n",
        "data3 = read_test_sets(test_path, img_size, classes, validation_size=0)\n",
        "\n",
        "\n",
        "print(\"Size of:\")\n",
        "print(\"- Training-set:\\t\\t{}\".format(len(data.train.labels)))\n",
        "print(\"- Validation-set:\\t{}\".format(len(data2.valid.labels)))\n",
        "print(\"- Test-set:\\t\\t{}\".format(len(data3.test.labels)))"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Reading training images\n",
            "Loading 1.Cancer files (Index: 0)\n",
            "Loading 2.Precancer files (Index: 1)\n",
            "Loading 3.Inflammatory files (Index: 2)\n",
            "Loading 4.Normal files (Index: 3)\n",
            "Reading validation images\n",
            "Loading 1.Cancer files (Index: 0)\n",
            "Loading 2.Precancer files (Index: 1)\n",
            "Loading 3.Inflammatory files (Index: 2)\n",
            "Loading 4.Normal files (Index: 3)\n",
            "Reading validation images\n",
            "Loading 1.Cancer files (Index: 0)\n",
            "Loading 2.Precancer files (Index: 1)\n",
            "Loading 3.Inflammatory files (Index: 2)\n",
            "Loading 4.Normal files (Index: 3)\n",
            "Size of:\n",
            "- Training-set:\t\t4038\n",
            "- Validation-set:\t392\n",
            "- Test-set:\t\t392\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jpaEjHGTgR1p"
      },
      "source": [
        "def new_weights(shape):\r\n",
        "    return tf.Variable(tf.truncated_normal(shape, stddev=0.05))\r\n",
        "    #tf.truncated_normal: 너무 작거나 너무 큰 값이 아닌 값으로 랜덤한 값을 가져오는 것\r\n",
        "\r\n",
        "def new_biases(length):\r\n",
        "    return tf.Variable(tf.constant(0.05, shape=[length]))"
      ],
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "n1Mb098qj28C"
      },
      "source": [
        "def new_conv_layer(input,              \n",
        "                   num_input_channels, \n",
        "                   filter_size,       \n",
        "                   num_filters,       \n",
        "                   use_pooling=True): \n",
        "\n",
        "    shape = [filter_size, filter_size, num_input_channels, num_filters]\n",
        "\n",
        "    weights = new_weights(shape=shape)\n",
        "\n",
        "    biases = new_biases(length=num_filters)\n",
        "\n",
        "    layer = tf.nn.conv2d(input=input,\n",
        "                         filter=weights,\n",
        "                         strides=[1, 1, 1, 1],\n",
        "                         padding='SAME')\n",
        "\n",
        "    layer += biases\n",
        "\n",
        "    layer = tf.nn.conv2d(input=input,\n",
        "                         filter=weights,\n",
        "                         strides=[1, 1, 1, 1],\n",
        "                         padding='SAME')\n",
        "\n",
        "    layer += biases\n",
        "\n",
        "    if use_pooling:\n",
        "\n",
        "        layer = tf.nn.max_pool(value=layer,\n",
        "                               ksize=[1, 2, 2, 1],\n",
        "                               strides=[1, 2, 2, 1],\n",
        "                               padding='SAME')\n",
        "\n",
        "    layer = tf.nn.relu(layer)\n",
        "\n",
        "\n",
        "    return layer, weights\n",
        "\n",
        "def new_conv_layer_drop(input,              \n",
        "                   num_input_channels, \n",
        "                   filter_size,       \n",
        "                   num_filters,       \n",
        "                   use_pooling=True): \n",
        "\n",
        "    shape = [filter_size, filter_size, num_input_channels, num_filters]\n",
        "\n",
        "    weights = new_weights(shape=shape)\n",
        "\n",
        "    biases = new_biases(length=num_filters)\n",
        "\n",
        "    layer = tf.nn.conv2d(input=input,\n",
        "                         filter=weights,\n",
        "                         strides=[1, 1, 1, 1],\n",
        "                         padding='SAME')\n",
        "\n",
        "    layer += biases\n",
        "\n",
        "    layer = tf.nn.conv2d(input=input,\n",
        "                         filter=weights,\n",
        "                         strides=[1, 1, 1, 1],\n",
        "                         padding='SAME')\n",
        "\n",
        "    layer += biases\n",
        "\n",
        "    if use_pooling:\n",
        "\n",
        "        layer = tf.nn.max_pool(value=layer,\n",
        "                               ksize=[1, 2, 2, 1],\n",
        "                               strides=[1, 2, 2, 1],\n",
        "                               padding='SAME')\n",
        "\n",
        "    layer = tf.nn.relu(layer)\n",
        "    layer = tf.nn.dropout(layer, keep_prob=keep_prob)\n",
        "    \n",
        "    return layer, weights\n",
        "\n",
        "\n",
        "def flatten_layer(layer):\n",
        "\n",
        "    layer_shape = layer.get_shape()\n",
        "\n",
        "    num_features = layer_shape[1:8].num_elements()\n",
        "    \n",
        "    layer_flat = tf.reshape(layer, [-1, num_features])\n",
        "\n",
        "    return layer_flat, num_features\n",
        "\n",
        "  \n",
        "def new_fc_layer(input,        \n",
        "                 num_inputs,    \n",
        "                 num_outputs,  \n",
        "                 use_relu=True):\n",
        "\n",
        "    weights = new_weights(shape=[num_inputs, num_outputs])\n",
        "    biases = new_biases(length=num_outputs)\n",
        "\n",
        "    layer = tf.matmul(input, weights) + biases\n",
        "    \n",
        "    if use_relu:\n",
        "        layer = tf.nn.relu(layer)\n",
        "\n",
        "    return layer\n",
        "\n",
        "\n",
        "\n",
        "x = tf.compat.v1.placeholder(tf.float32, shape=[None, img_size_flat], name='x')\n",
        "\n",
        "x_image = tf.reshape(x, [-1, img_size, img_size, num_channels])\n",
        "\n",
        "y_true = tf.compat.v1.placeholder(tf.float32, shape=[None, num_classes], name='y_true')\n",
        "\n",
        "y_true_cls = tf.argmax(y_true, axis=1)\n",
        "\n",
        "keep_prob = tf.compat.v1.placeholder(tf.float32)"
      ],
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CDZiWgIMj3G_",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "96ebc890-5bd6-44d4-9f3d-ee100e381fda"
      },
      "source": [
        "#######################################################################\\\n",
        "layer_conv1, weights_conv1 = \\\n",
        "    new_conv_layer_drop(input=x_image,\n",
        "                   num_input_channels=num_channels,\n",
        "                   filter_size=filter_size1,\n",
        "                   num_filters=num_filters1,\n",
        "                   use_pooling=True)\n",
        "     \n",
        "layer_conv2, weights_conv2 = \\\n",
        "    new_conv_layer(input=layer_conv1,\n",
        "                   num_input_channels=num_filters1,\n",
        "                   filter_size=filter_size2,\n",
        "                   num_filters=num_filters2,\n",
        "                   use_pooling=True)\n",
        "    \n",
        "layer_conv3, weights_conv3 = \\\n",
        "    new_conv_layer_drop(input=layer_conv2,\n",
        "                   num_input_channels=num_filters2,\n",
        "                   filter_size=filter_size3,\n",
        "                   num_filters=num_filters3,\n",
        "                   use_pooling=True)\n",
        "    \n",
        "layer_conv4, weights_conv4 = \\\n",
        "    new_conv_layer(input=layer_conv3,\n",
        "                   num_input_channels=num_filters3,\n",
        "                   filter_size=filter_size4,\n",
        "                   num_filters=num_filters4,\n",
        "                   use_pooling=True)\n",
        "    \n",
        "layer_conv5, weights_conv5 = \\\n",
        "    new_conv_layer_drop(input=layer_conv4,\n",
        "                   num_input_channels=num_filters4,\n",
        "                   filter_size=filter_size5,\n",
        "                   num_filters=num_filters5,\n",
        "                   use_pooling=True)\n",
        "    \n",
        "layer_conv6, weights_conv6 = \\\n",
        "    new_conv_layer(input=layer_conv5,\n",
        "                   num_input_channels=num_filters5,\n",
        "                   filter_size=filter_size6,\n",
        "                   num_filters=num_filters6,\n",
        "                   use_pooling=True)\n",
        "    \n",
        "layer_conv7, weights_conv7 = \\\n",
        "    new_conv_layer_drop(input=layer_conv6,\n",
        "                   num_input_channels=num_filters6,\n",
        "                   filter_size=filter_size7,\n",
        "                   num_filters=num_filters7,\n",
        "                   use_pooling=True)\n",
        "\n",
        "layer_conv8, weights_conv8 = \\\n",
        "    new_conv_layer_drop(input=layer_conv7,\n",
        "                   num_input_channels=num_filters7,\n",
        "                   filter_size=filter_size8,\n",
        "                   num_filters=num_filters8,\n",
        "                   use_pooling=True)\n",
        "     \n",
        "layer_conv9, weights_conv9 = \\\n",
        "    new_conv_layer(input=layer_conv8,\n",
        "                   num_input_channels=num_filters8,\n",
        "                   filter_size=filter_size9,\n",
        "                   num_filters=num_filters9,\n",
        "                   use_pooling=True)\n",
        "    \n",
        "layer_conv10, weights_conv10 = \\\n",
        "    new_conv_layer_drop(input=layer_conv9,\n",
        "                   num_input_channels=num_filters9,\n",
        "                   filter_size=filter_size10,\n",
        "                   num_filters=num_filters10,\n",
        "                   use_pooling=True)\n",
        "    \n",
        "layer_conv11, weights_conv11 = \\\n",
        "    new_conv_layer(input=layer_conv10,\n",
        "                   num_input_channels=num_filters10,\n",
        "                   filter_size=filter_size11,\n",
        "                   num_filters=num_filters11,\n",
        "                   use_pooling=True)\n",
        "    \n",
        "layer_conv12, weights_conv12 = \\\n",
        "    new_conv_layer_drop(input=layer_conv11,\n",
        "                   num_input_channels=num_filters11,\n",
        "                   filter_size=filter_size12,\n",
        "                   num_filters=num_filters12,\n",
        "                   use_pooling=True)\n",
        "    \n",
        "layer_conv13, weights_conv13 = \\\n",
        "    new_conv_layer(input=layer_conv12,\n",
        "                   num_input_channels=num_filters12,\n",
        "                   filter_size=filter_size13,\n",
        "                   num_filters=num_filters13,\n",
        "                   use_pooling=True)\n",
        "\n",
        "\n",
        "\n",
        "layer_flat, num_features = flatten_layer(layer_conv13)\n",
        "\n",
        "\n",
        "\n",
        "layer_fc1 = new_fc_layer(input=layer_flat,\n",
        "                         num_inputs=num_features,\n",
        "                         num_outputs=fc_size,\n",
        "                         use_relu=True)\n",
        "\n",
        "layer_fc2 = new_fc_layer(input=layer_fc1,\n",
        "                         num_inputs=fc_size,\n",
        "                         num_outputs=128,\n",
        "                         use_relu=False)\n",
        "\n",
        "layer_fc3 = new_fc_layer(input=layer_fc2,\n",
        "                         num_inputs=128,\n",
        "                         num_outputs=num_classes,\n",
        "                         use_relu=False)\n",
        "\n",
        "'''layer_fc4 = new_fc_layer(input=layer_fc3,\n",
        "                         num_inputs=64,\n",
        "                         num_outputs=32,\n",
        "                         use_relu=False)\n",
        "\n",
        "layer_fc5 = new_fc_layer(input=layer_fc4,\n",
        "                         num_inputs=32,\n",
        "                         num_outputs=16,\n",
        "                         use_relu=False)\n",
        "\n",
        "layer_fc6 = new_fc_layer(input=layer_fc5,\n",
        "                         num_inputs=16,\n",
        "                         num_outputs=num_classes,                        \n",
        "                         use_relu=False)'''\n",
        "\n",
        "#######################################################################\n",
        "\n",
        "\n",
        "y_pred = tf.nn.softmax(layer_fc3)\n",
        "\n",
        "y_pred_cls = tf.argmax(y_pred, axis=1)\n",
        "\n",
        "loss_func = tf.nn.softmax_cross_entropy_with_logits_v2(logits=layer_fc3, labels=y_true)\n",
        "\n",
        "cost = tf.reduce_mean(loss_func)\n",
        "\n",
        "regularizer = tf.nn.l2_loss(weights_conv13)\n",
        "beta = 0.001\n",
        "cost = tf.reduce_mean(cost + beta*regularizer)\n",
        "\n",
        "optimizer = tf.train.AdamOptimizer(learning_rate=1e-4).minimize(cost)\n",
        "\n",
        "correct_prediction = tf.equal(y_pred_cls, y_true_cls)\n",
        "\n",
        "accuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow/python/util/dispatch.py:201: calling dropout (from tensorflow.python.ops.nn_ops) with keep_prob is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Please use `rate` instead of `keep_prob`. Rate should be set to `rate = 1 - keep_prob`.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "C1bwlIuRj3Rt",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "fd172319-0b04-4edf-e250-2b2eb1b53c0b"
      },
      "source": [
        "from sklearn.linear_model import RidgeClassifier\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import OrdinalEncoder, LabelEncoder\n",
        "from yellowbrick.classifier import ROCAUC"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/sklearn/utils/deprecation.py:144: FutureWarning: The sklearn.metrics.classification module is  deprecated in version 0.22 and will be removed in version 0.24. The corresponding classes / functions should instead be imported from sklearn.metrics. Anything that cannot be imported from sklearn.metrics is now part of the private API.\n",
            "  warnings.warn(message, FutureWarning)\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Vp7NT_sktKSO"
      },
      "source": [
        "각 epoch당 학습 결과 표시"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VC2FqbukrX5a"
      },
      "source": [
        "session = tf.Session()\n",
        "\n",
        "session.run(tf.global_variables_initializer())\n",
        "\n",
        "train_batch_size = batch_size\n",
        "\n",
        "save_dir=\"/content/drive/Shareddrives/CTRC/model\"\n",
        "\n",
        "def print_progress(epoch, feed_dict_train, val_loss):\n",
        "    acc = session.run(accuracy, feed_dict=feed_dict_train)\n",
        "    msg = \"Epoch {0} --- Training Accuracy: {1:>6.1%},  Validation Loss: {2:.3f}\"\n",
        "    os.path.join(save_dir, \"epoch{epoch:03d}_valloss{val_loss:0.4f}.hdf5\")\n",
        "    print(msg.format(epoch + 1, acc, val_loss))\n",
        "\n",
        "    if epoch <= 47:\n",
        "      print(print_validation_accuracy(show_confusion_matrix=False))\n",
        "      print(print_test_accuracy(show_confusion_matrix=False))\n",
        "    else:\n",
        "      print(print_validation_accuracy(show_confusion_matrix=True))\n",
        "      print(print_test_accuracy(show_confusion_matrix=True))\n",
        "\n",
        "\n",
        "total_iterations = 0"
      ],
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FmCAkkBQtah9"
      },
      "source": [
        "train data 학습"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Omo5o5ketaFe"
      },
      "source": [
        "def optimize(num_iterations):\n",
        "    global total_iterations\n",
        "    \n",
        "\n",
        "    start_time = time.time()\n",
        "    \n",
        "    best_val_loss = float(\"inf\")\n",
        "    patience = 0\n",
        "\n",
        "    for i in range(total_iterations,\n",
        "                   total_iterations + num_iterations):\n",
        "\n",
        "        \n",
        "\n",
        "        x_batch, y_true_batch, _, cls_batch = data.train.next_batch(train_batch_size)\n",
        "        x_valid_batch, y_valid_batch, _, valid_cls_batch = data2.valid.next_batch(train_batch_size)\n",
        "        x_test_batch, y_test_batch, _, test_cls_batch = data3.test.next_batch(train_batch_size)\n",
        "\n",
        "        x_batch = x_batch.reshape(train_batch_size, img_size_flat)\n",
        "        x_valid_batch = x_valid_batch.reshape(train_batch_size, img_size_flat)\n",
        "        x_test_batch = x_test_batch.reshape(train_batch_size, img_size_flat)\n",
        "\n",
        "        feed_dict_train = {x: x_batch,\n",
        "                           y_true: y_true_batch, keep_prob: 0.70}\n",
        "        \n",
        "        feed_dict_validate = {x: x_valid_batch,\n",
        "                              y_true: y_valid_batch, keep_prob : 0.70}\n",
        "\n",
        "        feed_dict_test = {x: x_test_batch,\n",
        "                          y_true: y_test_batch, keep_prob : 0.70}\n",
        "\n",
        "        session.run(optimizer, feed_dict=feed_dict_train)\n",
        "        \n",
        "        if i % int(data.train.num_examples/batch_size) == 0: \n",
        "            val_loss = session.run(cost, feed_dict=feed_dict_validate)\n",
        "            epoch = int(i / int(data.train.num_examples/batch_size))\n",
        "\n",
        "            print_progress(epoch, feed_dict_train, val_loss)\n",
        "            \n",
        "            if early_stopping:    \n",
        "                if val_loss < best_val_loss:\n",
        "                    best_val_loss = val_loss\n",
        "                    patience = 0\n",
        "                else:\n",
        "                    patience += 1\n",
        "\n",
        "                if patience == early_stopping:\n",
        "                    break\n",
        "\n",
        "\n",
        "    total_iterations += num_iterations\n",
        "\n",
        "    end_time = time.time()\n",
        "\n",
        "    time_dif = end_time - start_time\n",
        "\n",
        "    print(\"Time elapsed: \" + str(timedelta(seconds=int(round(time_dif)))))"
      ],
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VuxQbVVeuV-_"
      },
      "source": [
        "validation accuracy"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eXOOM3ajuVH9"
      },
      "source": [
        "def print_validation_accuracy(show_example_errors=False, show_confusion_matrix=False):\n",
        "\n",
        "    num_test = len(data2.valid.images)\n",
        "\n",
        "    cls_pred = np.zeros(shape=num_test, dtype=np.int)\n",
        "\n",
        "    i = 0\n",
        "\n",
        "    while i < num_test:\n",
        " \n",
        "        j = min(i + batch_size, num_test)\n",
        "\n",
        "        images = data2.valid.images[i:j, :].reshape(-1, img_size_flat)\n",
        "        \n",
        "        labels = data2.valid.labels[i:j, :]\n",
        "\n",
        "        feed_dict = {x: images, y_true: labels, keep_prob: 0.70}\n",
        "\n",
        "        cls_pred[i:j] = session.run(y_pred_cls, feed_dict=feed_dict)\n",
        "\n",
        "        i = j\n",
        "\n",
        "\n",
        "    cls_true = np.array(data2.valid.cls)\n",
        "    cls_pred = np.array([classes[x] for x in cls_pred])\n",
        "\n",
        " \n",
        "\n",
        "    correct = (cls_true == cls_pred)\n",
        "\n",
        "    correct_sum = correct.sum()\n",
        "\n",
        "    acc = float(correct_sum) / num_test\n",
        "\n",
        "    msg = \"Validation Accuracy: {0:.1%} ({1} / {2})\"\n",
        "    print(msg.format(acc, correct_sum, num_test))\n",
        "\n",
        "    if show_example_errors:\n",
        "        print(\"Example errors:\")\n",
        "        plot_example_errors(cls_pred=cls_pred, correct=correct)\n",
        "\n",
        "    if show_confusion_matrix:\n",
        "        print(\"Confusion Matrix:\")\n",
        "        plot_confusion_matrix(cls_pred=cls_pred)"
      ],
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9NDCx8SDub_O"
      },
      "source": [
        "test accuracy"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ou-zRKfJujcy"
      },
      "source": [
        "def print_test_accuracy(show_example_errors=False, show_confusion_matrix=False):\n",
        "\n",
        "    num_test2 = len(data3.test.images)\n",
        "\n",
        "    cls_pred2 = np.zeros(shape=num_test2, dtype=np.int)\n",
        "\n",
        "    i = 0\n",
        "\n",
        "    while i < num_test2:\n",
        " \n",
        "        j = min(i + batch_size, num_test2)\n",
        "\n",
        "        images = data3.test.images[i:j, :].reshape(-1, img_size_flat)\n",
        "        \n",
        "        labels = data3.test.labels[i:j, :]\n",
        "\n",
        "        feed_dict = {x: images, y_true: labels, keep_prob: 0.70}\n",
        "\n",
        "        cls_pred2[i:j] = session.run(y_pred_cls, feed_dict=feed_dict)\n",
        "\n",
        "        i = j\n",
        "\n",
        "\n",
        "    cls_true2 = np.array(data3.test.cls)\n",
        "    cls_pred2 = np.array([classes[x] for x in cls_pred2])\n",
        "\n",
        " \n",
        "\n",
        "    correct = (cls_true2 == cls_pred2)\n",
        "\n",
        "    correct_sum = correct.sum()\n",
        "\n",
        "    acc = float(correct_sum) / num_test2\n",
        "\n",
        "    msg = \"Test Accuracy: {0:.1%} ({1} / {2})\"\n",
        "    print(msg.format(acc, correct_sum, num_test2))\n",
        "\n",
        "    if show_example_errors:\n",
        "        print(\"Example errors:\")\n",
        "        plot_example_errors(cls_pred2=cls_pred2, correct=correct)\n",
        "\n",
        "    if show_confusion_matrix:\n",
        "        print(\"Confusion Matrix:\")\n",
        "        plot_confusion_matrix2(cls_pred2=cls_pred2)"
      ],
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rDu8NXYjulqZ"
      },
      "source": [
        "from sklearn.metrics import roc_auc_score, recall_score, accuracy_score, f1_score\n",
        "from sklearn.metrics import roc_curve"
      ],
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7F6Zevqbunfc"
      },
      "source": [
        "혼동 행렬 1 (validation)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CXobAw7mupuD"
      },
      "source": [
        "def plot_confusion_matrix(cls_pred):\n",
        "    \n",
        "    cls_true = data2.valid.cls\n",
        "\n",
        "    cm = confusion_matrix(y_true=cls_true, y_pred=cls_pred)\n",
        "    \n",
        "    print(cm)\n",
        "    \n",
        "    plt.matshow(cm, cmap=plt.cm.Wistia_r)\n",
        "\n",
        "    plt.colorbar()\n",
        "    tick_marks = np.arange(num_classes)\n",
        "    plt.xticks(tick_marks, range(num_classes))\n",
        "    plt.yticks(tick_marks, range(num_classes))\n",
        "\n",
        "    plt.xlabel('Predicted')\n",
        "    plt.ylabel('True')\n",
        "    for i in range(0,4):\n",
        "      for j in range(0,4):\n",
        "        plt.text(j,i,str(cm[i][j]))\n",
        "    plt.show()\n",
        "\n",
        "    print(\"1.Cancer의 정밀도 : \", (cm[0,0]/(cm[0,0]+cm[1,0]+cm[2,0]+cm[3,0]))*100, \"%\")\n",
        "    print(\"1.Cancer의 재현율 : \", (cm[0,0]/(cm[0,0]+cm[0,1]+cm[0,2]+cm[0,3]))*100, \"%\")\n",
        "    print(\"2.Precancer의 정밀도 : \", (cm[1,1]/(cm[0,1]+cm[1,1]+cm[2,1]+cm[3,1]))*100, \"%\")\n",
        "    print(\"2.Precancer의 재현율 : \", (cm[1,1]/(cm[1,0]+cm[1,1]+cm[1,2]+cm[1,3]))*100, \"%\")\n",
        "    print(\"3.Inflammatory의 정밀도 : \", (cm[2,2]/(cm[0,2]+cm[1,2]+cm[2,2]+cm[3,2]))*100, \"%\")\n",
        "    print(\"3.Inflammatory의 재현율 : \", (cm[2,2]/(cm[2,0]+cm[2,1]+cm[2,2]+cm[2,3]))*100, \"%\")\n",
        "    print(\"4.Normal의 정밀도 : \", (cm[3,3]/(cm[0,3]+cm[1,3]+cm[2,3]+cm[3,3]))*100, \"%\")\n",
        "    print(\"4.Normal의 재현율 : \", (cm[3,3]/(cm[3,0]+cm[3,1]+cm[3,2]+cm[3,3]))*100, \"%\")"
      ],
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "grwAER5DuxLe"
      },
      "source": [
        "혼동 행렬 2 + ROC (test)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CjCjCHEAu0H8"
      },
      "source": [
        "def plot_confusion_matrix2(cls_pred2):\n",
        "    \n",
        "    cls_true2 = data3.test.cls\n",
        "\n",
        "    cm = confusion_matrix(y_true=cls_true2, y_pred=cls_pred2)\n",
        "    \n",
        "    print(cm)\n",
        "    \n",
        "    plt.matshow(cm, cmap=plt.cm.Wistia_r)\n",
        "\n",
        "    plt.colorbar()\n",
        "    \n",
        "    plt.title('Test Confusion Matrix')\n",
        "    tick_marks = np.arange(num_classes)\n",
        "    plt.xticks(tick_marks, range(num_classes))\n",
        "    plt.yticks(tick_marks, range(num_classes))\n",
        "    plt.grid(False)\n",
        "\n",
        "    plt.xlabel('Predicted')\n",
        "    plt.ylabel('True')\n",
        "    for i in range(0,4):\n",
        "      for j in range(0,4):\n",
        "        plt.text(j,i,str(cm[i][j]))\n",
        "    plt.show()\n",
        "\n",
        "    print(\"1.Cancer의 정밀도 : \", (cm[0,0]/(cm[0,0]+cm[1,0]+cm[2,0]+cm[3,0]))*100, \"%\")\n",
        "    print(\"1.Cancer의 재현율 : \", (cm[0,0]/(cm[0,0]+cm[0,1]+cm[0,2]+cm[0,3]))*100, \"%\")\n",
        "    print(\"2.Precancer의 정밀도 : \", (cm[1,1]/(cm[0,1]+cm[1,1]+cm[2,1]+cm[3,1]))*100, \"%\")\n",
        "    print(\"2.Precancer의 재현율 : \", (cm[1,1]/(cm[1,0]+cm[1,1]+cm[1,2]+cm[1,3]))*100, \"%\")\n",
        "    print(\"3.Inflammatory의 정밀도 : \", (cm[2,2]/(cm[0,2]+cm[1,2]+cm[2,2]+cm[3,2]))*100, \"%\")\n",
        "    print(\"3.Inflammatory의 재현율 : \", (cm[2,2]/(cm[2,0]+cm[2,1]+cm[2,2]+cm[2,3]))*100, \"%\")\n",
        "    print(\"4.Normal의 정밀도 : \", (cm[3,3]/(cm[0,3]+cm[1,3]+cm[2,3]+cm[3,3]))*100, \"%\")\n",
        "    print(\"4.Normal의 재현율 : \", (cm[3,3]/(cm[3,0]+cm[3,1]+cm[3,2]+cm[3,3]))*100, \"%\")\n",
        "\n",
        "    \n",
        "    cls_true2 = cls_true2.tolist()\n",
        "    cls_pred2 = cls_pred2.tolist()\n",
        "\n",
        "    real_true1 = []\n",
        "    real_pred1 = []\n",
        "\n",
        "    real_true2 = []\n",
        "    real_pred2 = []\n",
        "\n",
        "    real_true3 = []\n",
        "    real_pred3 = []\n",
        "\n",
        "    real_true4 = []\n",
        "    real_pred4 = []\n",
        "\n",
        "    i=0\n",
        "    j=0\n",
        "    k=0\n",
        "    l=0\n",
        "\n",
        "    for i in range(len(cls_true2)):\n",
        "      if cls_true2[i] == \"1.Cancer\":\n",
        "        real_true1.append(1)\n",
        "        if cls_true2[i] == cls_pred2[i]:\n",
        "          real_pred1.append(1)\n",
        "        else:\n",
        "          real_pred1.append(0)\n",
        "\n",
        "\n",
        "    for j in range(len(cls_true2)):\n",
        "      if cls_true2[j] == \"1.Cancer\":\n",
        "        real_true2.append(1)\n",
        "        if cls_true2[j] == cls_pred2[j]:\n",
        "          real_pred2.append(1)\n",
        "        else:\n",
        "          real_pred2.append(0)\n",
        "\n",
        "\n",
        "    for k in range(len(cls_true2)):\n",
        "      if cls_true2[k] == \"1.Cancer\":\n",
        "        real_true3.append(1)\n",
        "        if cls_true2[k] == cls_pred2[k]:\n",
        "          real_pred3.append(1)\n",
        "        else:\n",
        "          real_pred3.append(0)\n",
        "   \n",
        "\n",
        "    for l in range(len(cls_true2)):\n",
        "      if cls_true2[l] == \"1.Cancer\":\n",
        "        real_true4.append(1)\n",
        "        if cls_true2[l] == cls_pred2[l]:\n",
        "          real_pred4.append(1)\n",
        "        else:\n",
        "          real_pred4.append(0)\n",
        "    \n",
        "\n",
        "    real_true1 = np.array(real_true1)\n",
        "    real_pred1 = np.array(real_pred1)\n",
        "\n",
        "    real_true2 = np.array(real_true2)\n",
        "    real_pred2 = np.array(real_pred2)\n",
        "\n",
        "    real_true3 = np.array(real_true3)\n",
        "    real_pred3 = np.array(real_pred3)\n",
        "\n",
        "    real_true4 = np.array(real_true4)\n",
        "    real_pred4 = np.array(real_pred4)\n",
        "\n",
        "\n",
        "    print( \"Cancer_roc_auc : \", roc_auc_score(real_true1, real_pred1, multi_class='ovr', average='micro'))\n",
        "    print( \"Precancer_roc_auc : \", roc_auc_score(real_true2, real_pred2, multi_class='ovr', average='micro'))\n",
        "    print( \"Inflammatory_roc_auc : \", roc_auc_score(real_true3, real_pred3, multi_class='ovr', average='micro'))\n",
        "    print( \"Normal_roc_auc : \", roc_auc_score(real_true4, real_pred4, multi_class='ovr', average='micro'))\n",
        "\n",
        "\n",
        "    can = roc_auc_score(real_true1, real_pred1, multi_class='ovr', average='micro')\n",
        "    pre = roc_auc_score(real_true2, real_pred2, multi_class='ovr', average='micro')\n",
        "    inf = roc_auc_score(real_true3, real_pred3, multi_class='ovr', average='micro')\n",
        "    nor = roc_auc_score(real_true4, real_pred4, multi_class='ovr', average='micro')\n",
        "\n",
        "\n",
        "\n",
        "    fpr1, tpr1, threshold1 = roc_curve(real_true1, real_pred1)\n",
        "    fpr2, tpr2, threshold2 = roc_curve(real_true2, real_pred2)\n",
        "    fpr3, tpr3, threshold3 = roc_curve(real_true3, real_pred3)\n",
        "    fpr4, tpr4, threshold4 = roc_curve(real_true4, real_pred4)\n",
        "\n",
        "\n",
        "    plt.plot(fpr1, tpr1, '-', label=\"Cancer ROC Curve = %f\"%(can), color='r')\n",
        "    plt.plot(fpr2, tpr2, '-', label=\"Precancer ROC Curve = %f\"%(pre), color='m')\n",
        "    plt.plot(fpr3, tpr3, '-', label=\"Inflammatory ROC Curve = %f\"%(inf), color='g')\n",
        "    plt.plot(fpr4, tpr4, '-', label=\"Normal ROC Curve = %f\"%(nor), color='b')\n",
        "    plt.plot([0,1],[0,1], 'k--')\n",
        "    plt.axis([0,1,0,1])\n",
        "\n",
        "    plt.xlabel(\"fpr\")\n",
        "    plt.ylabel(\"tpr\")\n",
        "    plt.legend(loc='lower right')\n",
        "    plt.show()"
      ],
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_EBnOecZu-Fl"
      },
      "source": [
        "실행"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lQedM6MWj3lD",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "ae7f3eb5-589a-4565-e736-c40a571844af"
      },
      "source": [
        "optimize(num_iterations=1000000)"
      ],
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1 --- Training Accuracy:  46.9%,  Validation Loss: 2.225\n",
            "Validation Accuracy: 57.1% (224 / 392)\n",
            "None\n",
            "Test Accuracy: 54.8% (215 / 392)\n",
            "None\n",
            "Epoch 2 --- Training Accuracy:  46.9%,  Validation Loss: 2.104\n",
            "Validation Accuracy: 57.9% (227 / 392)\n",
            "None\n",
            "Test Accuracy: 57.9% (227 / 392)\n",
            "None\n",
            "Epoch 3 --- Training Accuracy:  46.9%,  Validation Loss: 1.641\n",
            "Validation Accuracy: 57.9% (227 / 392)\n",
            "None\n",
            "Test Accuracy: 57.9% (227 / 392)\n",
            "None\n",
            "Epoch 4 --- Training Accuracy:  46.9%,  Validation Loss: 1.876\n",
            "Validation Accuracy: 57.9% (227 / 392)\n",
            "None\n",
            "Test Accuracy: 57.9% (227 / 392)\n",
            "None\n",
            "Epoch 5 --- Training Accuracy:  46.9%,  Validation Loss: 1.456\n",
            "Validation Accuracy: 57.9% (227 / 392)\n",
            "None\n",
            "Test Accuracy: 57.9% (227 / 392)\n",
            "None\n",
            "Epoch 6 --- Training Accuracy:  46.9%,  Validation Loss: 1.717\n",
            "Validation Accuracy: 57.9% (227 / 392)\n",
            "None\n",
            "Test Accuracy: 57.9% (227 / 392)\n",
            "None\n",
            "Epoch 7 --- Training Accuracy:  46.9%,  Validation Loss: 1.252\n",
            "Validation Accuracy: 57.9% (227 / 392)\n",
            "None\n",
            "Test Accuracy: 57.9% (227 / 392)\n",
            "None\n",
            "Epoch 8 --- Training Accuracy:  46.9%,  Validation Loss: 1.511\n",
            "Validation Accuracy: 57.9% (227 / 392)\n",
            "None\n",
            "Test Accuracy: 57.9% (227 / 392)\n",
            "None\n",
            "Epoch 9 --- Training Accuracy:  46.9%,  Validation Loss: 1.162\n",
            "Validation Accuracy: 58.2% (228 / 392)\n",
            "None\n",
            "Test Accuracy: 58.2% (228 / 392)\n",
            "None\n",
            "Epoch 10 --- Training Accuracy:  46.9%,  Validation Loss: 1.464\n",
            "Validation Accuracy: 57.9% (227 / 392)\n",
            "None\n",
            "Test Accuracy: 57.9% (227 / 392)\n",
            "None\n",
            "Epoch 11 --- Training Accuracy:  46.9%,  Validation Loss: 1.053\n",
            "Validation Accuracy: 57.9% (227 / 392)\n",
            "None\n",
            "Test Accuracy: 58.2% (228 / 392)\n",
            "None\n",
            "Epoch 12 --- Training Accuracy:  46.9%,  Validation Loss: 1.333\n",
            "Validation Accuracy: 58.2% (228 / 392)\n",
            "None\n",
            "Test Accuracy: 57.9% (227 / 392)\n",
            "None\n",
            "Epoch 13 --- Training Accuracy:  43.8%,  Validation Loss: 0.895\n",
            "Validation Accuracy: 58.7% (230 / 392)\n",
            "None\n",
            "Test Accuracy: 58.7% (230 / 392)\n",
            "None\n",
            "Epoch 14 --- Training Accuracy:  46.9%,  Validation Loss: 1.212\n",
            "Validation Accuracy: 64.3% (252 / 392)\n",
            "None\n",
            "Test Accuracy: 65.8% (258 / 392)\n",
            "None\n",
            "Epoch 15 --- Training Accuracy:  59.4%,  Validation Loss: 0.877\n",
            "Validation Accuracy: 65.3% (256 / 392)\n",
            "None\n",
            "Test Accuracy: 69.4% (272 / 392)\n",
            "None\n",
            "Epoch 16 --- Training Accuracy:  56.2%,  Validation Loss: 1.152\n",
            "Validation Accuracy: 66.1% (259 / 392)\n",
            "None\n",
            "Test Accuracy: 65.6% (257 / 392)\n",
            "None\n",
            "Epoch 17 --- Training Accuracy:  56.2%,  Validation Loss: 0.832\n",
            "Validation Accuracy: 65.3% (256 / 392)\n",
            "None\n",
            "Test Accuracy: 64.3% (252 / 392)\n",
            "None\n",
            "Epoch 18 --- Training Accuracy:  50.0%,  Validation Loss: 1.114\n",
            "Validation Accuracy: 65.6% (257 / 392)\n",
            "None\n",
            "Test Accuracy: 63.8% (250 / 392)\n",
            "None\n",
            "Epoch 19 --- Training Accuracy:  53.1%,  Validation Loss: 0.729\n",
            "Validation Accuracy: 68.1% (267 / 392)\n",
            "None\n",
            "Test Accuracy: 64.8% (254 / 392)\n",
            "None\n",
            "Epoch 20 --- Training Accuracy:  56.2%,  Validation Loss: 1.103\n",
            "Validation Accuracy: 67.1% (263 / 392)\n",
            "None\n",
            "Test Accuracy: 64.0% (251 / 392)\n",
            "None\n",
            "Epoch 21 --- Training Accuracy:  56.2%,  Validation Loss: 0.730\n",
            "Validation Accuracy: 68.6% (269 / 392)\n",
            "None\n",
            "Test Accuracy: 64.8% (254 / 392)\n",
            "None\n",
            "Epoch 22 --- Training Accuracy:  56.2%,  Validation Loss: 1.028\n",
            "Validation Accuracy: 67.6% (265 / 392)\n",
            "None\n",
            "Test Accuracy: 63.0% (247 / 392)\n",
            "None\n",
            "Epoch 23 --- Training Accuracy:  53.1%,  Validation Loss: 0.674\n",
            "Validation Accuracy: 68.9% (270 / 392)\n",
            "None\n",
            "Test Accuracy: 65.3% (256 / 392)\n",
            "None\n",
            "Epoch 24 --- Training Accuracy:  53.1%,  Validation Loss: 1.042\n",
            "Validation Accuracy: 68.4% (268 / 392)\n",
            "None\n",
            "Test Accuracy: 62.8% (246 / 392)\n",
            "None\n",
            "Epoch 25 --- Training Accuracy:  53.1%,  Validation Loss: 0.748\n",
            "Validation Accuracy: 66.6% (261 / 392)\n",
            "None\n",
            "Test Accuracy: 65.6% (257 / 392)\n",
            "None\n",
            "Epoch 26 --- Training Accuracy:  56.2%,  Validation Loss: 1.035\n",
            "Validation Accuracy: 68.4% (268 / 392)\n",
            "None\n",
            "Test Accuracy: 63.8% (250 / 392)\n",
            "None\n",
            "Epoch 27 --- Training Accuracy:  53.1%,  Validation Loss: 0.598\n",
            "Validation Accuracy: 68.6% (269 / 392)\n",
            "None\n",
            "Test Accuracy: 63.8% (250 / 392)\n",
            "None\n",
            "Epoch 28 --- Training Accuracy:  53.1%,  Validation Loss: 1.162\n",
            "Validation Accuracy: 63.5% (249 / 392)\n",
            "None\n",
            "Test Accuracy: 64.3% (252 / 392)\n",
            "None\n",
            "Epoch 29 --- Training Accuracy:  50.0%,  Validation Loss: 0.665\n",
            "Validation Accuracy: 65.3% (256 / 392)\n",
            "None\n",
            "Test Accuracy: 65.1% (255 / 392)\n",
            "None\n",
            "Epoch 30 --- Training Accuracy:  53.1%,  Validation Loss: 1.014\n",
            "Validation Accuracy: 70.4% (276 / 392)\n",
            "None\n",
            "Test Accuracy: 64.5% (253 / 392)\n",
            "None\n",
            "Epoch 31 --- Training Accuracy:  53.1%,  Validation Loss: 0.747\n",
            "Validation Accuracy: 67.3% (264 / 392)\n",
            "None\n",
            "Test Accuracy: 64.8% (254 / 392)\n",
            "None\n",
            "Epoch 32 --- Training Accuracy:  56.2%,  Validation Loss: 1.013\n",
            "Validation Accuracy: 68.1% (267 / 392)\n",
            "None\n",
            "Test Accuracy: 65.1% (255 / 392)\n",
            "None\n",
            "Epoch 33 --- Training Accuracy:  50.0%,  Validation Loss: 0.680\n",
            "Validation Accuracy: 64.8% (254 / 392)\n",
            "None\n",
            "Test Accuracy: 65.3% (256 / 392)\n",
            "None\n",
            "Epoch 34 --- Training Accuracy:  56.2%,  Validation Loss: 0.958\n",
            "Validation Accuracy: 70.9% (278 / 392)\n",
            "None\n",
            "Test Accuracy: 63.0% (247 / 392)\n",
            "None\n",
            "Epoch 35 --- Training Accuracy:  53.1%,  Validation Loss: 0.682\n",
            "Validation Accuracy: 67.3% (264 / 392)\n",
            "None\n",
            "Test Accuracy: 64.3% (252 / 392)\n",
            "None\n",
            "Epoch 36 --- Training Accuracy:  59.4%,  Validation Loss: 0.956\n",
            "Validation Accuracy: 69.4% (272 / 392)\n",
            "None\n",
            "Test Accuracy: 65.1% (255 / 392)\n",
            "None\n",
            "Epoch 37 --- Training Accuracy:  62.5%,  Validation Loss: 0.687\n",
            "Validation Accuracy: 70.2% (275 / 392)\n",
            "None\n",
            "Test Accuracy: 66.1% (259 / 392)\n",
            "None\n",
            "Epoch 38 --- Training Accuracy:  53.1%,  Validation Loss: 0.913\n",
            "Validation Accuracy: 67.3% (264 / 392)\n",
            "None\n",
            "Test Accuracy: 65.8% (258 / 392)\n",
            "None\n",
            "Epoch 39 --- Training Accuracy:  56.2%,  Validation Loss: 0.698\n",
            "Validation Accuracy: 65.8% (258 / 392)\n",
            "None\n",
            "Test Accuracy: 65.3% (256 / 392)\n",
            "None\n",
            "Epoch 40 --- Training Accuracy:  59.4%,  Validation Loss: 0.854\n",
            "Validation Accuracy: 71.7% (281 / 392)\n",
            "None\n",
            "Test Accuracy: 63.3% (248 / 392)\n",
            "None\n",
            "Epoch 41 --- Training Accuracy:  50.0%,  Validation Loss: 0.630\n",
            "Validation Accuracy: 67.6% (265 / 392)\n",
            "None\n",
            "Test Accuracy: 66.1% (259 / 392)\n",
            "None\n",
            "Epoch 42 --- Training Accuracy:  56.2%,  Validation Loss: 0.872\n",
            "Validation Accuracy: 69.6% (273 / 392)\n",
            "None\n",
            "Test Accuracy: 66.6% (261 / 392)\n",
            "None\n",
            "Epoch 43 --- Training Accuracy:  53.1%,  Validation Loss: 0.661\n",
            "Validation Accuracy: 65.3% (256 / 392)\n",
            "None\n",
            "Test Accuracy: 65.8% (258 / 392)\n",
            "None\n",
            "Epoch 44 --- Training Accuracy:  62.5%,  Validation Loss: 0.965\n",
            "Validation Accuracy: 68.9% (270 / 392)\n",
            "None\n",
            "Test Accuracy: 67.3% (264 / 392)\n",
            "None\n",
            "Epoch 45 --- Training Accuracy:  59.4%,  Validation Loss: 0.641\n",
            "Validation Accuracy: 69.1% (271 / 392)\n",
            "None\n",
            "Test Accuracy: 68.4% (268 / 392)\n",
            "None\n",
            "Epoch 46 --- Training Accuracy:  62.5%,  Validation Loss: 0.868\n",
            "Validation Accuracy: 69.1% (271 / 392)\n",
            "None\n",
            "Test Accuracy: 67.9% (266 / 392)\n",
            "None\n",
            "Epoch 47 --- Training Accuracy:  62.5%,  Validation Loss: 0.623\n",
            "Validation Accuracy: 70.9% (278 / 392)\n",
            "None\n",
            "Test Accuracy: 67.3% (264 / 392)\n",
            "None\n",
            "Epoch 48 --- Training Accuracy:  62.5%,  Validation Loss: 0.977\n",
            "Validation Accuracy: 70.4% (276 / 392)\n",
            "None\n",
            "Test Accuracy: 68.9% (270 / 392)\n",
            "None\n",
            "Epoch 49 --- Training Accuracy:  65.6%,  Validation Loss: 0.567\n",
            "Validation Accuracy: 70.2% (275 / 392)\n",
            "Confusion Matrix:\n",
            "[[ 56   0   0  26]\n",
            " [ 15   0   0  15]\n",
            " [ 18   0   0  35]\n",
            " [  8   0   0 219]]\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 396x396 with 2 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "1.Cancer의 정밀도 :  57.73195876288659 %\n",
            "1.Cancer의 재현율 :  68.29268292682927 %\n",
            "2.Precancer의 정밀도 :  nan %\n",
            "2.Precancer의 재현율 :  0.0 %\n",
            "3.Inflammatory의 정밀도 :  nan %\n",
            "3.Inflammatory의 재현율 :  0.0 %\n",
            "4.Normal의 정밀도 :  74.23728813559322 %\n",
            "4.Normal의 재현율 :  96.47577092511013 %\n",
            "None\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:25: RuntimeWarning: invalid value encountered in long_scalars\n",
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:27: RuntimeWarning: invalid value encountered in long_scalars\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Test Accuracy: 68.1% (267 / 392)\n",
            "Confusion Matrix:\n",
            "[[ 53   0   0  29]\n",
            " [ 17   0   0  13]\n",
            " [ 28   0   0  25]\n",
            " [ 13   0   0 214]]\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 396x396 with 2 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "1.Cancer의 정밀도 :  47.74774774774775 %\n",
            "1.Cancer의 재현율 :  64.63414634146342 %\n",
            "2.Precancer의 정밀도 :  nan %\n",
            "2.Precancer의 재현율 :  0.0 %\n",
            "3.Inflammatory의 정밀도 :  nan %\n",
            "3.Inflammatory의 재현율 :  0.0 %\n",
            "4.Normal의 정밀도 :  76.15658362989323 %\n",
            "4.Normal의 재현율 :  94.27312775330397 %\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:28: RuntimeWarning: invalid value encountered in long_scalars\n",
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:30: RuntimeWarning: invalid value encountered in long_scalars\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "error",
          "ename": "ValueError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-19-3af8205977c0>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0moptimize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnum_iterations\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1000000\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m<ipython-input-13-aa3d6e40c1d5>\u001b[0m in \u001b[0;36moptimize\u001b[0;34m(num_iterations)\u001b[0m\n\u001b[1;32m     36\u001b[0m             \u001b[0mepoch\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mi\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0mint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnum_examples\u001b[0m\u001b[0;34m/\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     37\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 38\u001b[0;31m             \u001b[0mprint_progress\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mepoch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mval_loss\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     39\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     40\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mearly_stopping\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-12-119fb6e953c4>\u001b[0m in \u001b[0;36mprint_progress\u001b[0;34m(epoch, feed_dict_train, val_loss)\u001b[0m\n\u001b[1;32m     18\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     19\u001b[0m       \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprint_validation_accuracy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mshow_confusion_matrix\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 20\u001b[0;31m       \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprint_test_accuracy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mshow_confusion_matrix\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     21\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     22\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-15-f0c4b979fd97>\u001b[0m in \u001b[0;36mprint_test_accuracy\u001b[0;34m(show_example_errors, show_confusion_matrix)\u001b[0m\n\u001b[1;32m     42\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mshow_confusion_matrix\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     43\u001b[0m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Confusion Matrix:\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 44\u001b[0;31m         \u001b[0mplot_confusion_matrix2\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcls_pred2\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcls_pred2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m<ipython-input-18-6fa48bf54090>\u001b[0m in \u001b[0;36mplot_confusion_matrix2\u001b[0;34m(cls_pred2)\u001b[0m\n\u001b[1;32m    103\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    104\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 105\u001b[0;31m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m \u001b[0;34m\"Cancer_roc_auc : \"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mroc_auc_score\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mreal_true1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreal_pred1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmulti_class\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'ovr'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maverage\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'micro'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    106\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m \u001b[0;34m\"Precancer_roc_auc : \"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mroc_auc_score\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mreal_true2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreal_pred2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmulti_class\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'ovr'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maverage\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'micro'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    107\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m \u001b[0;34m\"Inflammatory_roc_auc : \"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mroc_auc_score\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mreal_true3\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreal_pred3\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmulti_class\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'ovr'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maverage\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'micro'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/sklearn/metrics/_ranking.py\u001b[0m in \u001b[0;36mroc_auc_score\u001b[0;34m(y_true, y_score, average, sample_weight, max_fpr, multi_class, labels)\u001b[0m\n\u001b[1;32m    388\u001b[0m                                              max_fpr=max_fpr),\n\u001b[1;32m    389\u001b[0m                                      \u001b[0my_true\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_score\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maverage\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 390\u001b[0;31m                                      sample_weight=sample_weight)\n\u001b[0m\u001b[1;32m    391\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# multilabel-indicator\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    392\u001b[0m         return _average_binary_score(partial(_binary_roc_auc_score,\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/sklearn/metrics/_base.py\u001b[0m in \u001b[0;36m_average_binary_score\u001b[0;34m(binary_metric, y_true, y_score, average, sample_weight)\u001b[0m\n\u001b[1;32m     75\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     76\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0my_type\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"binary\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 77\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mbinary_metric\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_true\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_score\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msample_weight\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msample_weight\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     78\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     79\u001b[0m     \u001b[0mcheck_consistent_length\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_true\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_score\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msample_weight\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/sklearn/metrics/_ranking.py\u001b[0m in \u001b[0;36m_binary_roc_auc_score\u001b[0;34m(y_true, y_score, sample_weight, max_fpr)\u001b[0m\n\u001b[1;32m    219\u001b[0m     \u001b[0;34m\"\"\"Binary roc auc score\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    220\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munique\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_true\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 221\u001b[0;31m         raise ValueError(\"Only one class present in y_true. ROC AUC score \"\n\u001b[0m\u001b[1;32m    222\u001b[0m                          \"is not defined in that case.\")\n\u001b[1;32m    223\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mValueError\u001b[0m: Only one class present in y_true. ROC AUC score is not defined in that case."
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OC8MqRlchEGH"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}
